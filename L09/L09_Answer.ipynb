{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/L09/L09_Answer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `SST_DP2025`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f4c2-8506-415c-a295-041ee40d9eda",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 09\n",
    "\n",
    "- LangChain is a framework built around LLMs.\n",
    "- Framework offered as a Python or Javascript (Typescript) package.\n",
    "- Use it to build chatbots, Generative Question-Answer (GQA), summarization and much more.\n",
    "- Core idea is to “chain” together different components to create more advanced use cases around LLMs.\n",
    "- Provides developers with a comprehensive set of tools to seamlessly combine multiple prompts working with LLMs effortlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a344c6-9289-4351-9677-8e7f778c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langgraph\n",
    "%pip install --quiet -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f2767-9f87-48e8-b1b7-c0b952d2cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain        0.3.11\n",
    "# langgraph        0.2.59\n",
    "# langchain-core   0.3.24\n",
    "# langchain-openai 0.2.12\n",
    "# openai           1.57.2\n",
    "# pydantic         2.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7281bdb-a238-4cad-abac-6679aa169ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc02a79-8283-4235-b570-c813d51bff91",
   "metadata": {},
   "source": [
    "## A Simple LLM Application\n",
    "We will be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models). It takes a sequence of message as inputs and returns chat messages as outputs.\n",
    "\n",
    "LangChain does not host any of the chat models. It depends on [third party](https://python.langchain.com/docs/integrations/chat/) LLM model providers. We will be using ChatOpenAI due to its popularity and performant. There are a few standard parameters that we can set with the chat models. Two most common are:\n",
    "- `model`: the name of the model\n",
    "- `temperature`: the sampling temperature\n",
    "\n",
    "`temperature` controls the randomness or creativity of the model's output. Low temperature (close to 0) is more deterministic and focused outputs. High temperature (close to 1) is good for creative tasks or generating varied responses.\n",
    "\n",
    "Chat models in LangChain have a number of (default methods)[https://python.langchain.com/v0.2/docs/concepts/#runnable-interface]. Most of the time, we will be using:\n",
    "- `stream`: stream back chunks of response\n",
    "- `invoke`: call the chain on the input\n",
    "  \n",
    "Chat models take messages as input. Messages have a role that describes who is saying the message and a content property. We get an `AIMessage` response upon invoking the model with messages.\n",
    "\n",
    "We can also invoke a chat model with a string. The string is converted to `HumanMessage` and then passed to the model for processing. This interface is consistent across all chat models and models are typically initialised once at the start of each notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892ce11d-96fc-4aad-b307-0ec50122e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2fe6ff-5fba-46d9-a05d-ec835ce93b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chat LLM model\n",
    "chat_model = ChatOpenAI(\n",
    "    # don't need this if the OpenAI API Key is stored in the environment variable\n",
    "    #openai_api_key=\"sk-proj-xxxxxxxxx\",\n",
    "\n",
    "    model = 'gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f524decd-83e2-47ac-a9c0-8ce81fe1946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup message prompt\n",
    "text = \"What date is Singapore National Day?\"\n",
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2ebcf8-6c39-432a-b0a5-13eefbfd9ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore National Day is celebrated on August 9th each year. This day marks the country's independence from Malaysia in 1965.\n"
     ]
    }
   ],
   "source": [
    "# note that Chat Model takes in message objects as input and generate message object as output\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324918b-ae6e-49ae-8eb3-edbd8569817d",
   "metadata": {},
   "source": [
    "## A Simple LLM Application With Prompt Template\n",
    "\n",
    "**Purpose of Prompt Template**\n",
    "- Parameterized templates that dynamically generate specific prompts based on user input or other variables.\n",
    "- By using variables in the template, you can adapt the prompt to specific scenarios without rewriting the entire prompt.\n",
    "- Keeps your application logic (like API calls or response handling) separate from the prompt text, improving code readability and maintainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53266ed8-b053-4872-8622-a5949dda4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba721d4-9855-4c15-a188-ff90b396068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise ChatModel with API key\n",
    "chat_model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini', \n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c572494-7a0a-4df2-a7e1-e7b9a55f1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template takes in raw user input ({input_language} and {output_language}) and \n",
    "# return a prompt that is ready to pass into a language model\n",
    "\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "\n",
    "human_template = \"{text}\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2005be55-3b7f-45d6-bc9e-a7538c7d0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime la programmation.\n"
     ]
    }
   ],
   "source": [
    "# trsnslate English to French\n",
    "\n",
    "messages = chat_prompt.format_messages(\n",
    "    input_language = \"English\", \n",
    "    output_language = \"French\", \n",
    "    text = \"I love programming.\"\n",
    ")\n",
    "\n",
    "response = chat_model.invoke(messages).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da80a83a-d814-4c23-9aa8-9efa33ee7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我热爱编程。\n"
     ]
    }
   ],
   "source": [
    "# translate English to Chinese\n",
    "\n",
    "messages = chat_prompt.format_messages(\n",
    "    input_language = \"English\", \n",
    "    output_language = \"Chinese\", \n",
    "    text = \"I love programming.\"\n",
    ")\n",
    "\n",
    "response = chat_model.invoke(messages).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7f5ef-ebaa-4fae-a9d0-d79254b910f6",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "`Runnable` interface is the foundation for working with LangChain components. A unit of work that can be invoked, batched, streamed, transformed and composed.\n",
    "\n",
    "**Key Methods - (Synchronouse/Asynchronous):**\n",
    "- `invoke`/`ainvoke`: Transforms a single input into an output.\n",
    "- `batch`/`abatch`: Transforms multiple inputs into outputs.\n",
    "- `stream`/`astream`: Outputs are streamed as they are produced.\n",
    "\n",
    "Streaming is typically used when interfacting with language models to enable real-time or chunked responses. The purpose of this parameter is to allow the model to stream its output piece by piece (e.g., word by word or token by token) instead of waiting for the entire response to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bc2f1fa-35e1-48bc-925f-0a35c686d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = chat_prompt.format_messages(\n",
    "    input_language = \"English\", \n",
    "    output_language = \"English\", \n",
    "    text = \"What is teh-c siew dai?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c69e880-6403-460c-a996-e388192a19fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Teh-c siew dai\" is a term from Hokkien, a Chinese dialect commonly spoken in Southeast Asia, particularly in Malaysia and Singapore. It refers to a type of tea, specifically \"tea with milk\" (teh) that is served with less sugar (siew dai). \"Siew dai\" means \"less sweet\" in Hokkien. So, when you order \"teh-c siew dai,\" you are asking for a cup of tea with milk that is not very sweet."
     ]
    }
   ],
   "source": [
    "# observation:\n",
    "# the output is streamed/printed piece by piece (eg. word by word or token by token)\n",
    "\n",
    "for token in chat_model.stream(messages):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7520039-1b31-46ca-96da-f5af86facff2",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "- From the sample codes you just run, you have learned how to create your first simple LLM application. \n",
    "- You learned how to work with language model(s) and how to create a prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6301b-ede6-4653-879e-8c47cc0b0d2b",
   "metadata": {},
   "source": [
    "## PromptTemplate And LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e694c058-eb8e-4cfa-bf5c-28ebc0e75280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f05f62e3-cd4c-44a5-8f90-313755229e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4 does not have 'instruct' model\n",
    "# The chat models like gpt-4 and gpt-4-turbo are already instruction-following models.\n",
    "# They are designed to interpret and respond effectively to instructions provided in the conversatio\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.7,\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d692ef24-ee42-4564-87ba-0002b5b191c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup template \n",
    "\n",
    "human_template = \"Write {lines} sentences about {topic}.\"\n",
    "prompt = ChatPromptTemplate.from_template(human_template)\n",
    "\n",
    "lines_topic_dict = {\n",
    "    \"lines\" : \"3\", \n",
    "    \"topic\": \"Sir Stamford Raffles\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc8889c-e7f3-4096-a4df-14880c1a0bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sir Stamford Raffles was a British colonial administrator and the founder of modern Singapore, establishing it as a strategic trading post in 1819. He played a pivotal role in the expansion of British influence in Southeast Asia and was instrumental in the development of various social and educational reforms in the region. Raffles is also known for his interest in natural history and for his efforts to preserve the biodiversity of the Malay Archipelago.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 83, 'prompt_tokens': 17, 'total_tokens': 100, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-add99593-2032-44d4-9a38-278e353e628c-0', usage_metadata={'input_tokens': 17, 'output_tokens': 83, 'total_tokens': 100, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without piping to StrOutputParser\n",
    "# StrOutputParser = OutputParser that parses LLMResult into the top likely string\n",
    "\n",
    "lcel_chain_01 = prompt | llm\n",
    "\n",
    "lcel_chain_01.invoke(lines_topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d3fca9-f12f-4eb8-9e29-1f4fd13f91e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sir Stamford Raffles was a British statesman and the founder of modern Singapore, establishing it as a vital trading post in 1819. He played a crucial role in expanding British influence in Southeast Asia and was instrumental in the establishment of the British East India Company's control over the region. Additionally, Raffles was a noted naturalist and contributed significantly to the study of the flora and fauna of the Malay Archipelago.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipe to StrOutputParesr\n",
    "\n",
    "lcel_chain_02 = prompt | llm | output_parser\n",
    "\n",
    "lcel_chain_02.invoke(lines_topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32fd62-6e98-4294-8b46-42d8d452eee6",
   "metadata": {},
   "source": [
    "## Template\n",
    "\n",
    "### Prompt Template\n",
    "Prompt templates take as input a dictionary where each key represents a variable for the prompt template to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dbbd33f-8d1e-40c9-8ecf-e39c8c36a40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='I love playing table-tennis')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    input_variables = [\"sport\"],\n",
    "    template = \"I love playing {sport}\"\n",
    ")\n",
    "\n",
    "prompt.invoke({\"sport\":\"table-tennis\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ecd88-7b23-460d-b31c-02f3bb64e4a4",
   "metadata": {},
   "source": [
    "Usually you will initialise Prompts using `from_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33087d42-d382-46ef-8092-7b97e2883de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='I love visitng Japan because of its scenery')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"I love visitng {country} because of its {adjective}\"\n",
    ")\n",
    "\n",
    "prompt.invoke({\n",
    "    \"country\" : \"Japan\",\n",
    "    \"adjective\" : \"scenery\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad56989-900d-4625-bda6-17483a6c3804",
   "metadata": {},
   "source": [
    "### MessagesPlaceholder\n",
    "Prompt template is responsible for adding a list of messages in a particular place. If the user want to pass in a list of messages that could be slotted into a particular spot, we can use `MessagePlaceholder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e7c6452-6998-4cf6-824d-c6adac2a7bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi! My name is John', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "\n",
    "    # place Human Message after System Message\n",
    "    MessagesPlaceholder(\"messages\")\n",
    "])\n",
    "\n",
    "prompt.invoke({\"messages\" : [HumanMessage(content=\"Hi! My name is John\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907223f-3e31-4f2e-8037-cd4a54330226",
   "metadata": {},
   "source": [
    "## LangChain Parsers / Structured Output\n",
    "\n",
    "LangChain API Reference: [LangChain output_parser](https://python.langchain.com/api_reference/langchain/output_parsers.html)\n",
    "\n",
    "### CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "498b3688-c28e-4cb1-a8ef-e7fd8896c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.list import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a36ef677-c3fc-433e-a364-787880fd2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4696597f-5a79-489b-b266-8f2bfebc9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"List down 5 countries that start with letter'{alphabet}'\\n{format_instructions}\",\n",
    "    input_variables=[\"alphabet\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cdebfe0-b34b-46f4-9845-2c3061042161",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ff65cf3-3ba9-4290-a67a-e53b1def1327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain, Sweden, Switzerland, Singapore, South Africa\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(alphabet=\"S\")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3255e8-ff14-44c2-a05a-f88baa86b9ce",
   "metadata": {},
   "source": [
    "## JSON\n",
    "JSON is a lightweight, human-readable format for representing structured data. LLMs often use JSON to exchange information in a structured and consistent way. JSON is commonly used to represent and transmit structured data, making it easier to process and use in programming tasks.\n",
    "\n",
    "JSON can represent key-value pairs, lists, and nested objects, which are useful for structured outputs. Many APIs, including those of LLMs, send and receive data in JSON format. LLMs might generate JSON outputs to integrate with other systems, such as databases, web applications, or scripts.\n",
    "\n",
    "### Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b46537b-8eb5-4ce1-b01d-dbfcb5e00eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    model_kwargs = {\"response_format\" : { \"type\": \"json_object\" } }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d68df024-89cd-4f0c-aa9c-e397993bade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"\"\"\n",
    "   Return a JSON object with two variables. The \"name\" is \"Joe Doe\" and his \"age\" is \"30\".\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fe80fae-5e92-469c-bf60-40c4eea15ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Joe Doe\",\n",
      "  \"age\": \"30\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f72af-4510-48fc-b99d-f6279ba73a1e",
   "metadata": {},
   "source": [
    "### Method 2:\n",
    "\n",
    "- JSON requires `{` and `}` for its syntax. To escape these in Python strings, use double braces {{ and }}.\n",
    "- `{name}` and `{age}` placeholders for variables remain single braces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b03db585-97a6-4207-9b30-58734a7ea5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"name\", \"age\"],\n",
    "    template=\"\"\"You are an assistant that formats responses in JSON.\n",
    "Given the following inputs:\n",
    "- Name: {name}\n",
    "- Age: {age}\n",
    "\n",
    "Respond with a JSON object in the following format:\n",
    "{{\n",
    "    \"name\": \"value\",\n",
    "    \"age\": value\n",
    "}}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7db1c103-6b0f-48cb-808b-c3e58a830328",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(name=\"John Doe\", age=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c59e6a6-fcc0-4dd9-bc26-714dbf7c8741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"John Doe\",\n",
      "    \"age\": 30\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc8aea-ad2f-4202-a6bd-a59d01421eac",
   "metadata": {},
   "source": [
    "## Schema Definition\n",
    "\n",
    "- The output structure of a model response needs to be represented in some way.\n",
    "- The simplest and most common format is a JSON-like structure which you just seen.\n",
    "- The other method is use `Pydantic` as it allow you mto define schemas using Python's type annotations.\n",
    "- It validates that the LLM's output conforms to the expected structure, catching errors early.\n",
    "\n",
    "### Method 1: (Tool)\n",
    "Tool challing refers to the mechanism where the language model interacts with external tools or functions to enhance its capabilities. This approach allows the model to perform actions, retrieve specific information, or execute tasks that go beyond its built-in knowledge and reasoning.\n",
    "\n",
    "For a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02a37201-8c76-474d-8b0a-ced95c50745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ResponseFormatter(BaseModel):\n",
    "    answer: str = Field(description = \"The answer to the user's question\")\n",
    "    followup_question: str = Field(description = \"A followup question the user could ask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb769fe7-a00d-477f-8bec-9f425940d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\", \n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# Bind responseformatter schema as a tool to the model\n",
    "model_with_tools = model.bind_tools([ResponseFormatter])\n",
    "\n",
    "# Invoke the model\n",
    "response = model_with_tools.invoke(\"What was the original colour of the Hulk in his first comic appearance?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "154f3594-0d48-42b3-92f0-8ac38a5bdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 'eval' method to extract followup_question\n",
    "\n",
    "followup = eval(response.additional_kwargs['tool_calls'][0]['function']['arguments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0a4cdf8-5e09-4012-b56e-9020bc81a115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why was the Hulk's color changed from gray to green?\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "followup['followup_question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05cd37-4584-44e9-a75a-f991e92bb337",
   "metadata": {},
   "source": [
    "### Method 2: (Pydantic)\n",
    "\n",
    "Specify an JSON schema and query LLM for JSON outputs that conform to that schema.\n",
    "\n",
    "Declare a data model with validation using Pydantic decoration `@validator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d20d0d72-58f5-4144-90fe-2f213b18fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.pydantic import PydanticOutputParser\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate\n",
    "\n",
    "class ResponseFormatter(BaseModel):\n",
    "    answer: str = Field(description = \"The answer to the user's question\")\n",
    "    followup_question: str = Field(description = \"A followup question the user could ask\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ResponseFormatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "777365ce-95c3-43be-aba2-81b1d4128eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\", \n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49fef609-ec20-45ec-9a18-6d4779ec28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "messages = prompt.format_prompt(\n",
    "    format_instructions=parser.get_format_instructions(),\n",
    "    query = \"What was the original colour of the Hulk in his first comic appearance?\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "933a5d02-0bc3-4178-9ade-02db9a6079e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7386a0c7-6240-4538-b3a9-29658ed944c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the parse to extract the answer and followup question\n",
    "\n",
    "output = parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "208eceb8-4ad5-46ed-abb0-5aea44ead1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The original color of the Hulk in his first comic appearance was gray.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38d3ff8c-44c1-4391-9aa5-baa6e1275f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What other colors has the Hulk been depicted in over the years?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.followup_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217b258-cfb1-43d4-a6ed-73a21bd9bc25",
   "metadata": {},
   "source": [
    "### Method 3: (ResponseSchema)\n",
    "\n",
    "Schema for a response from a structured output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3662bea5-85fa-4135-9cd4-d68e50bfc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.structured import ResponseSchema\n",
    "from langchain.output_parsers.structured import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b880c481-a9c8-4e56-8b5c-90faff7a8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the schemas\n",
    "\n",
    "answer_schema = ResponseSchema(\n",
    "    name = \"answer\",\n",
    "    description = \"The answer to the user's question\"\n",
    ")\n",
    "\n",
    "followup_question_schema = ResponseSchema(\n",
    "    name = \"followup_question\",\n",
    "    description = \"A follow up question the user could ask\"\n",
    ")\n",
    "\n",
    "response_schema = [\n",
    "    answer_schema,\n",
    "    followup_question_schema\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "557233ab-82cb-4cac-b2d6-aa02a68238d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // The answer to the user's question\n",
      "\t\"followup_question\": string  // A follow up question the user could ask\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# setup the output parser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# print the formatting instruction to understand the output format\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a37a2070-eddb-475a-adb6-04953a74c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "messages = prompt.format_prompt(\n",
    "    format_instructions=parser.get_format_instructions(),\n",
    "    query = \"What was the original colour of the Hulk in his first comic appearance?\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b6ba4cd-a37f-449a-a385-045b19bb07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the model\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "533d341f-eea1-48f7-b9a0-f13151c61d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"The original color of the Hulk in his first comic appearance was gray.\",\n",
      "  \"followup_question\": \"What other colors has the Hulk been depicted in over the years?\"\n",
      "}\n",
      "----------\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(response.content)\n",
    "print('-'*10)\n",
    "\n",
    "# the content is of type 'str'\n",
    "print(type(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ef00152-4610-45da-b0dc-93571d688044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force the output to be type dict\n",
    "\n",
    "dict_response = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de70490c-844e-4ef7-87e2-c8486f2a6da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'The original color of the Hulk in his first comic appearance was gray.', 'followup_question': 'What other colors has the Hulk been depicted in over the years?'}\n",
      "----------\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(dict_response)\n",
    "print('-'*10)\n",
    "\n",
    "# the content is of type 'str'\n",
    "print(type(dict_response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9061644-c5e1-4278-848d-db0628bbd096",
   "metadata": {},
   "source": [
    "## Memory Persistence\n",
    "- LLMs are stateless.\n",
    "- Each incoming query is processed independently.\n",
    "- Memory allows a LLM to remember previous interactions.\n",
    "\n",
    "According to LangChain documentation, it is recommended to take advantage of LangGraph persistence to incorporate `memory` into new LangChain applications.\n",
    "\n",
    "- [Build a Chatbot](https://python.langchain.com/docs/tutorials/chatbot/#installation)\n",
    "- [How to add thread-level persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
    "\n",
    "The `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationTokenBufferMemory` and `ConversationSummaryBufferMemory` methods are deprecated since version 0.3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "835323a6-b99b-4933-ba8c-81fbed905660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba088b14-eac4-491c-a2a7-ff0fb4c7191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1519129c-3fee-45f7-9605-8a1acdd04de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef1efa45-6499-47b4-a7c7-a56545b39f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# visualize the graph using the get_graph method and one of the \"draw\" methods, like draw_ascii or draw_png\n",
    "\n",
    "def drawGraph(graph):\n",
    "    try:\n",
    "        display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    except Exception:\n",
    "        # This requires some extra dependencies and is optional\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3073a6-146c-45e2-a8f3-a37e14c47715",
   "metadata": {},
   "source": [
    "### Without Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1356f350-d43a-4df7-a31d-eae53f2ed80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# define the (single) node in the graph\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3adca85a-f329-4c04-b50c-4c286bcb6e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw the graph\n",
    "drawGraph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92546fca-7d82-4886-be73-84e08bd8d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm John.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e22aa86-a40a-4dd1-9525-f24584e879a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi John! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb1e2f53-2d3c-4f4e-b7fa-856b6de9d8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi John! How can I assist you today?'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "646e4713-b23f-4dcc-8e65-f6f2e5787709",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68088a32-53b4-4c95-80fc-08f259a7e9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. If you'd like to tell me your name, feel free!\n"
     ]
    }
   ],
   "source": [
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a38e7-7269-4d43-ad18-c5cb7e585212",
   "metadata": {},
   "source": [
    "### With Persistence\n",
    "\n",
    "- To add in persistence, we need to pass in a `Checkpointer` when compiling the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "045024c9-b71c-475c-ad7d-dfc5b126bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ebdc0af-80f6-4b3c-ba9b-759fae876865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw the graph\n",
    "drawGraph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5738fffd-41ef-442b-af80-8ee5a7482333",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42c264ba-071b-4866-8d28-936c2205f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm John.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8d493ae-b75b-476e-8645-e5155c086da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi John! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b82b7274-3e53-45fd-aa14-ff37104877e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi John! How can I assist you today?'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a16e27e-8653-46b1-b78a-264bba7b5314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is John! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78112adc-afce-4670-8e08-f43e6f82655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is John! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d6f1f7-7e3b-4fef-9f8d-920ee61b6ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
