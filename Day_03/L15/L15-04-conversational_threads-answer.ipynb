{"cells":[{"cell_type":"markdown","source":["<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n","\n","[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/Day_03/L15/L15-04-conversational_threads-answer.ipynb)"],"metadata":{"id":"BQxtrpDrJoB0"}},{"cell_type":"markdown","source":[],"metadata":{"id":"LR6CWt2EJrIV"}},{"cell_type":"markdown","metadata":{"id":"jttHNJmA7Bmb"},"source":["# Conversational Threads"]},{"cell_type":"markdown","metadata":{"id":"pbuLmp9r7Bmd"},"source":["Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith.\n","\n","This is relevant to our RAG application, which should maintain context from prior conversations with users."]},{"cell_type":"markdown","metadata":{"id":"Vrp0LeD07Bmf"},"source":["### Setup"]},{"cell_type":"code","source":["%%capture --no-stderr\n","%pip install --quiet -U langchain\n","%pip install --quiet -U langgraph\n","%pip install --quiet -U langchain-openai\n","# %pip install --quiet -U grandalf\n","%pip install --quiet -U langchain-community\n","# %pip install --quiet -U faiss-cpu\n","# %pip install --quiet -U pytube\n","# %pip install --quiet -U youtube-transcript-api\n","%pip install --quiet -U python-dotenv"],"metadata":{"id":"pqJbtJ7p7vSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","# map google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","gdrive_path = \"/content/drive/My Drive/Colab Notebooks/00-LLM App Dev/Day 3\"\n","import os\n","os.chdir(gdrive_path)\n","sys.path.append(gdrive_path)\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MnUzPRaC7zpE","executionInfo":{"status":"ok","timestamp":1736059893679,"user_tz":-480,"elapsed":46561,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"028f28a8-c198-4454-d16c-4fddd278bb7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/00-LLM App Dev/Day 3\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Whx0pPI7Bmg"},"outputs":[],"source":["# You can set them inline\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = \"\"\n","os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n","os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-rp\"  # If you don't set this, traces will go to the Default project"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbXu9Dm27Bmh","executionInfo":{"status":"ok","timestamp":1736060333631,"user_tz":-480,"elapsed":5,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"8f825976-d669-4602-8d3a-776e5b8f9c98"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["# Or you can use a .env file\n","from dotenv import load_dotenv\n","load_dotenv(dotenv_path=\".env\", override=True)"]},{"cell_type":"code","source":["print(f'OpenAI API Key: {os.environ[\"OPENAI_API_KEY\"]}')\n","print(f'LangSmith API Key: {os.environ[\"LANGCHAIN_API_KEY\"]}')\n","print(f'Langsmith project: {os.environ[\"LANGCHAIN_PROJECT\"]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03Gv6q128KpV","executionInfo":{"status":"ok","timestamp":1736060213626,"user_tz":-480,"elapsed":4,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"1ae03ae9-e078-4e4c-f20f-269e7c2e2d8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["OpenAI API Key: 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\n","LangSmith API Key: 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\n","Langsmith project: langsmith-rp\n"]}]},{"cell_type":"markdown","metadata":{"id":"05DmjwlI7Bmi"},"source":["### Group traces into threads\n"]},{"cell_type":"markdown","metadata":{"id":"2nqOHUg17Bmj"},"source":["A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\n","\n","To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.\n","\n","The key value is the unique identifier for that conversation. The key name should be one of:\n","\n","- session_id\n","- thread_id\n","- conversation_id.\n","\n","The value should be a UUID."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdoJpa497Bmk"},"outputs":[],"source":["import uuid\n","thread_id = uuid.uuid4()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULrChPWG7Bml","executionInfo":{"status":"ok","timestamp":1736060394225,"user_tz":-480,"elapsed":53386,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"4f398af2-4ff8-478d-b2c6-6ae9df15a20a"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n","Fetching pages: 100%|##########| 219/219 [00:10<00:00, 20.46it/s]\n"]}],"source":["from langsmith import traceable\n","from openai import OpenAI\n","from typing import List\n","import nest_asyncio\n","from utils import get_vector_db_retriever\n","\n","openai_client = OpenAI()\n","nest_asyncio.apply()\n","retriever = get_vector_db_retriever()\n","\n","@traceable(run_type=\"chain\")\n","def retrieve_documents(question: str):\n","    return retriever.invoke(question)\n","\n","@traceable(run_type=\"chain\")\n","def generate_response(question: str, documents):\n","    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n","    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n","    Use the following pieces of retrieved context to answer the latest question in the conversation.\n","    If you don't know the answer, just say that you don't know.\n","    Use three sentences maximum and keep the answer concise.\n","    \"\"\"\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": rag_system_prompt\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n","        }\n","    ]\n","    return call_openai(messages)\n","\n","@traceable(run_type=\"llm\")\n","def call_openai(\n","    messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0\n",") -> str:\n","    return openai_client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature,\n","    )\n","\n","@traceable(run_type=\"chain\")\n","def langsmith_rag(question: str):\n","    documents = retrieve_documents(question)\n","    response = generate_response(question, documents)\n","    return response.choices[0].message.content\n"]},{"cell_type":"markdown","metadata":{"id":"u5Ll_quB7Bmm"},"source":["### Now let's run our application twice with this thread_id"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RzKZu5BY7Bmm","executionInfo":{"status":"ok","timestamp":1736060527990,"user_tz":-480,"elapsed":2921,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"29d2edbe-b234-4e96-ba7c-0cc843b721a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["To add metadata to a trace in LangSmith, you need to send arbitrary metadata and tags along with the trace. This can include information like the execution environment or the user who initiated the trace. For detailed instructions, refer to the documentation on adding metadata keys to a trace.\n"]}],"source":["question = \"How do I add metadata to a Trace?\"\n","ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n","print(ai_answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SymqHJ2H7Bmn","executionInfo":{"status":"ok","timestamp":1736060531923,"user_tz":-480,"elapsed":3937,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"a8851571-e88b-4233-ef4b-6be7a61e0451"},"outputs":[{"output_type":"stream","name":"stdout","text":["You can add tags to a trace in LangSmith by sending arbitrary metadata and tags along with the trace. Additionally, you can annotate a trace inline by clicking on the \"Annotate\" button in the upper right corner of the trace view or by sending it to the Annotation Queue. Feedback tags are associated with your workspace and can be used to critique specific parts of the trace.\n"]}],"source":["question = \"How can I add tags to a Trace?\"\n","ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"thread_id\": thread_id}})\n","print(ai_answer)"]},{"cell_type":"markdown","metadata":{"id":"jXFPN5oL7Bmn"},"source":["### Let's take a look in LangSmith!"]}],"metadata":{"kernelspec":{"display_name":"ls-academy","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}