{"cells":[{"cell_type":"markdown","source":["<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n","\n","[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/Day_03/L15/L15-01-tracing_basics-answer.ipynb)"],"metadata":{"id":"gC_5hWWyGXuG"}},{"cell_type":"markdown","metadata":{"id":"KcEVaX3Xp0h1"},"source":["# Tracing Basics"]},{"cell_type":"markdown","metadata":{"id":"z-KuC8j5p0h3"},"source":["### Setup"]},{"cell_type":"code","source":["%%capture --no-stderr\n","%pip install --quiet -U langchain\n","%pip install --quiet -U langgraph\n","%pip install --quiet -U langchain-openai\n","# %pip install --quiet -U grandalf\n","%pip install --quiet -U langchain-community\n","# %pip install --quiet -U faiss-cpu\n","# %pip install --quiet -U pytube\n","# %pip install --quiet -U youtube-transcript-api\n","%pip install --quiet -U python-dotenv\n"],"metadata":{"id":"kEQ_SrzLsUMp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXHDGXacp0h3"},"source":["Make sure you set your environment variables, including your OpenAI API key."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rkj5RNbnp0h4"},"outputs":[],"source":["# You can set them inline\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = \"\"\n","os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n","os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-rp\""]},{"cell_type":"code","source":["import sys\n","# map google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","gdrive_path = \"/content/drive/My Drive/Colab Notebooks/00-LLM App Dev/Day 3\"\n","import os\n","os.chdir(gdrive_path)\n","sys.path.append(gdrive_path)\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYVAurbarvrW","executionInfo":{"status":"ok","timestamp":1735782843435,"user_tz":-480,"elapsed":26684,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"ab7d662c-d74d-4d1e-8c82-d8f78e7b3797"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/00-LLM App Dev/Day 3\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45jWzIA7p0h4","executionInfo":{"status":"ok","timestamp":1735782849357,"user_tz":-480,"elapsed":409,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"43a90dad-b51a-42af-e7d5-6fc9287b8f29"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["# Or you can use a .env file\n","from dotenv import load_dotenv\n","load_dotenv(dotenv_path=\".env\", override=True)"]},{"cell_type":"code","source":["print(f'OpenAI API Key: {os.environ[\"OPENAI_API_KEY\"]}')\n","print(f'LangSmith API Key: {os.environ[\"LANGCHAIN_API_KEY\"]}')\n","print(f'Langsmith project: {os.environ[\"LANGCHAIN_PROJECT\"]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"opH3gPYDs1w8","executionInfo":{"status":"ok","timestamp":1735782853042,"user_tz":-480,"elapsed":327,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"f37dff82-d89b-4a96-dc30-7391b9dea366"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["OpenAI API Key: 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\n","LangSmith API Key: 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\n"]}]},{"cell_type":"markdown","metadata":{"id":"FUmwBXg4p0h5"},"source":["### Tracing with @traceable"]},{"cell_type":"markdown","metadata":{"id":"Ehz-PE44p0h6"},"source":["The @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\n","\n","The decorator works by creating a run tree for you each time the function is called and inserting it within the current trace. The function inputs, name, and other information is then streamed to LangSmith. If the function raises an error or if it returns a response, that information is also added to the tree, and updates are patched to LangSmith so you can detect and diagnose sources of errors. This is all done on a background thread to avoid blocking your app's execution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzIKvphcp0h6"},"outputs":[],"source":["# TODO: Import traceable (start)\n","from langsmith import traceable\n","# TODO: Import traceable (end)\n","\n","from openai import OpenAI\n","from typing import List\n","import nest_asyncio\n","from utils import get_vector_db_retriever\n","\n","MODEL_PROVIDER = \"openai\"\n","MODEL_NAME = \"gpt-4o-mini\"\n","APP_VERSION = 1.0\n","RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n","Use the following pieces of retrieved context to answer the latest question in the conversation.\n","If you don't know the answer, just say that you don't know.\n","Use three sentences maximum and keep the answer concise.\n","\"\"\"\n","\n","openai_client = OpenAI()\n","nest_asyncio.apply()\n","retriever = get_vector_db_retriever()\n","\n","# TODO: Set up tracing for each function (start)\n","@traceable\n","# TODO: Set up tracing for each function (end)\n","\n","def retrieve_documents(question: str):\n","    return retriever.invoke(question)   # NOTE: This is a LangChain vector db retriever, so this .invoke() call will be traced automatically\n","\n","# TODO: Set up tracing for each function (start)\n","@traceable\n","# TODO: Set up tracing for each function (end)\n","def generate_response(question: str, documents):\n","    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": RAG_SYSTEM_PROMPT\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n","        }\n","    ]\n","    return call_openai(messages)\n","\n","# TODO: Set up tracing for each function (start)\n","@traceable\n","# TODO: Set up tracing for each function (end)\n","def call_openai(\n","    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",") -> str:\n","    return openai_client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature,\n","    )\n","\n","# TODO: Set up tracing for each function (start)\n","@traceable\n","# TODO: Set up tracing for each function (end)\n","def langsmith_rag(question: str):\n","    documents = retrieve_documents(question)\n","    response = generate_response(question, documents)\n","    return response.choices[0].message.content\n"]},{"cell_type":"markdown","metadata":{"id":"6TVAp0xfp0h7"},"source":["@traceable handles the RunTree lifecycle for you!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cITB9rw-p0h7","executionInfo":{"status":"ok","timestamp":1735785984121,"user_tz":-480,"elapsed":2728,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"648d03b1-0cad-46c3-bde3-fac590f82774"},"outputs":[{"output_type":"stream","name":"stdout","text":["To trace with the @traceable decorator, simply decorate any function you want to log traces for by adding `@traceable` above its definition. Ensure that the LANGCHAIN_TRACING_V2 environment variable is set to 'true' and the LANGCHAIN_API_KEY is configured with your API key. This setup allows you to log traces effectively without modifying your code further.\n"]}],"source":["question = \"How can I trace with the @traceable decorator?\"\n","ai_answer = langsmith_rag(question)\n","print(ai_answer)"]},{"cell_type":"markdown","metadata":{"id":"2V4OxErqp0h7"},"source":["Let's take a look in LangSmith!"]},{"cell_type":"markdown","metadata":{"id":"mV-gmjNep0h8"},"source":["### Adding Metadata"]},{"cell_type":"markdown","metadata":{"id":"abZC-T2Lp0h8"},"source":["LangSmith supports sending arbitrary metadata along with traces.\n","\n","Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJvkgRpxp0h8"},"outputs":[],"source":["from langsmith import traceable\n","\n","@traceable(\n","    # TODO: Add Metadata\n","    metadata={\"vectordb\": \"sklearn\"}\n",")\n","def retrieve_documents(question: str):\n","    return retriever.invoke(question)\n","\n","@traceable\n","def generate_response(question: str, documents):\n","    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n","    messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": RAG_SYSTEM_PROMPT\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n","        }\n","    ]\n","    return call_openai(messages)\n","\n","@traceable(\n","    # TODO: Add Metadata\n","    metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER}\n",")\n","def call_openai(\n","    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",") -> str:\n","    return openai_client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature,\n","    )\n","\n","@traceable\n","def langsmith_rag(question: str):\n","    documents = retrieve_documents(question)\n","    response = generate_response(question, documents)\n","    return response.choices[0].message.content\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2J_5lScPp0h8","executionInfo":{"status":"ok","timestamp":1735788049131,"user_tz":-480,"elapsed":2265,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"4c9f7a68-e998-48be-d8ea-3b0dc0c752c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["To add metadata to a Run using the `@traceable` decorator, you can specify the metadata keys such as `session_id`, `thread_id`, or `conversation_id` when defining the traceable function. For detailed instructions, refer to the documentation on how to add metadata keys to a trace. This allows you to associate additional information with the trace effectively.\n"]}],"source":["question = \"How do I add Metadata to a Run with @traceable?\"\n","ai_answer = langsmith_rag(question)\n","print(ai_answer)"]},{"cell_type":"markdown","source":["Let's take a look in LangSmith!"],"metadata":{"id":"_GEgZdXrBawQ"}},{"cell_type":"markdown","metadata":{"id":"YIo75hI3p0h8"},"source":["You can also add metadata at runtime!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dpgf0ZjRp0h9","executionInfo":{"status":"ok","timestamp":1735788246407,"user_tz":-480,"elapsed":2051,"user":{"displayName":"Khee Wei Seow","userId":"09029520827574805999"}},"outputId":"7b49f52b-def8-4255-af4d-b68dd9b3c348"},"outputs":[{"output_type":"stream","name":"stdout","text":["You can add metadata at runtime by passing it in with the run ID using the `langsmith_extra` parameter. For example, you can call your function like this: `rag(\"your question\", langsmith_extra={\"run_id\": run_id, \"metadata\": {\"user_id\": \"harrison\"}})`. This allows you to log information such as a User ID dynamically during execution.\n"]}],"source":["question = \"How do I add metadata at runtime?\"\n","ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\n","print(ai_answer)"]},{"cell_type":"markdown","metadata":{"id":"XQP4bGfAp0h9"},"source":["Let's take a look in LangSmith!"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}