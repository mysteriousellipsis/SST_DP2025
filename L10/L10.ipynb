{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/L10/L10.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `SST_DP2025`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f4c2-8506-415c-a295-041ee40d9eda",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a344c6-9289-4351-9677-8e7f778c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langgraph\n",
    "%pip install --quiet -U langchain-openai\n",
    "%pip install --quiet -U grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f2767-9f87-48e8-b1b7-c0b952d2cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grandalf         0.8\n",
    "# langchain        0.3.11\n",
    "# langgraph        0.2.59\n",
    "# langchain-core   0.3.24\n",
    "# langchain-openai 0.2.12\n",
    "# openai           1.57.2\n",
    "# pydantic         2.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7281bdb-a238-4cad-abac-6679aa169ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc02a79-8283-4235-b570-c813d51bff91",
   "metadata": {},
   "source": [
    "## Chains\n",
    "\n",
    "### Basic/Entry Chain\n",
    "A basic chain or entry chain using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ce11d-96fc-4aad-b307-0ec50122e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9026c-14e7-4aff-a66e-8ffd748b61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"How do I say 'Hello' in {language}?\",\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fe6ff-5fba-46d9-a05d-ec835ce93b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42c119-53d9-4414-9b67-5e3b4bc8cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7ffc8-109e-4dd3-9f0f-5d1155c8d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: prompt -> model -> StrOutputParser\n",
    "chain = _____ | _____ | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a9775-1214-49a1-aa80-9c47dea06de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"language\" : \"French\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf73b80-34d2-40b6-91bc-3ef985f57750",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain (Single Input / Single Output)\n",
    "Simple chain where the output of one step feed directly into next. A streamlined version of a sequential chain, where the output of each step directly becomes the input for the next. It is perfect for straightforward workflows without the need for advanced memory or branching logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a819207-b750-42b5-8728-d1faa7a8de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95115df6-1c41-481d-af25-7c5ca0c3b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720a98b-6f4c-4572-b3f8-71bba8ad324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create prompt templates\n",
    "\n",
    "first_prompt = _____.from_template(\n",
    "    \"What is a good name for a company that specialises in making {product}?\"\n",
    ")\n",
    "\n",
    "second_prompt = ChatPromptTemplate._____(\n",
    "    \"Generate a short introduction of company: {company} .\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97035990-d579-43fd-82de-968242d54ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    first_prompt\n",
    "    | model\n",
    "    | {'company' : RunnablePassthrough()}   # 'company' is generated after the first prompt\n",
    "    | second_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ").with_types(input_type=Dict[str, str], output_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335cc210-fd6d-4595-a5f9-46fb22d01e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: invoke the chain by passing in 'Running Shoe' as product\n",
    "chain.invoke({'_____' : 'Running shoe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e842eb7-ead1-406c-b0d1-9e14a84caa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the chain as a graph\n",
    "\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31752660-b372-43ce-a40d-92fe8b7c399b",
   "metadata": {},
   "source": [
    "### Sequential Chain\n",
    "The `Sequential Chain` in LangChain is a simple way to execute a series of tasks in order, where the output of one step becomes the input for the next. This type of chain is ideal for workflows where the steps depend on one another in a linear sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89651e-78e4-419a-ba8c-6d8f94f6e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0af63-4534-4c44-aa46-e3119918f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a chat prompt template from teamplte\n",
    "first_prompt = _____._____(\n",
    "    \"Write a brief introduction about {topic}.\"\n",
    ")\n",
    "\n",
    "# TODO: first promppt -> model\n",
    "first_chain = _____ | _____ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4d835-98ee-4118-95af-f63bdaaae514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a chat prompt template from teamplte\n",
    "second_prompt = _____._____(\n",
    "    \"From this introduction: \\\"{introduction}\\\", generate three key bullet points.\"\n",
    ")\n",
    "\n",
    "# TODO: second prompt -> model\n",
    "second_chain = _____ | _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c28d93-c578-4ad2-852e-fe7f2bc16748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe in this prompt, it takes in two inputs 'introduction' and 'bullet_points' from earlier chains\n",
    "\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Combine the introduction: \\\"{introduction}\\\" with the key points: \\\"{bullet_points}\\\".\\n\"\n",
    "    \"Write a concise summary from this information.\"\n",
    ")\n",
    "\n",
    "# TODO: third prompt -> model\n",
    "third_chain = third_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc07d86-a2c0-4c1b-8355-d3c6f165198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"introduction\" : first_chain}\n",
    "    | RunnablePassthrough.assign(bullet_points=second_chain)\n",
    "    | RunnablePassthrough.assign(summary=third_chain)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d921243-c1a5-4d8e-bb60-4fa28fc40c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"topic\" : \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e70831-a7fb-466a-8a6f-f3bb65faefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['summary'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1653ff4-993b-4dee-9733-222a48f1ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfb617-f591-41d8-9f99-dde7ef7133ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting Runnables\n",
    "\n",
    "chain.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe0907-1b34-4225-ace0-6a917416bf29",
   "metadata": {},
   "source": [
    "### Routing between Sub-Chains\n",
    "This can be another form of router chain implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f79cf-802a-4630-a352-2f6f249f4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab798f5-29d9-4c10-9db3-6a8471eef6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"Given the user question below, classify it as either being about `Math`, `Science`, or `General`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6adbf-abcb-49fb-88a7-fa6ce58cbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24b781-eca5-4872-a0c4-a8f539785219",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc4164-ce0c-45b7-8d67-1db3d8cdbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and ensure the classification is working\n",
    "\n",
    "chain.invoke({\"question\" : \"What is the basic theory of quantum physics?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87acda92-1b6c-45d8-a573-df7e33aa0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a math chain\n",
    "\n",
    "math_chain = PromptTemplate.from_template(\n",
    "\"\"\"You are an expert in Mathematics.\n",
    "Always answer questions starting with \"As my MATH teacher told me\".\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d34e4-6485-47fb-b7f2-f598705dff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a science chain\n",
    "\n",
    "science_chain = PromptTemplate.from_template(\n",
    "\"\"\"You are an expert in Science.\n",
    "Always answer questions starting with \"As my SCIENCE teacher told me\".\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b9cf0-f18d-4f2a-a262-edbf1c5bedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generic chain\n",
    "\n",
    "general_chain = PromptTemplate.from_template(\n",
    "    \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62377ed-9269-464e-a073-84288cdf3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom function to route between different outputs\n",
    "\n",
    "# TODO: perform the routing according to info\n",
    "def route(info):\n",
    "    if \"math\" in info['topic'].lower():\n",
    "        return _____\n",
    "    elif \"science\" in info['topic'].lower():\n",
    "        return _____\n",
    "    else:\n",
    "        return _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fa1e0-4ab4-455a-a2c2-367ee324d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the full chain\n",
    "\n",
    "full_chain = (\n",
    "    {\"topic\" : chain, \"question\" : lambda x: x['question']} \n",
    "    | RunnableLambda (route)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff908df-8c28-48e0-ac65-6891ff90be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: invoke the chain\n",
    "response = full_chain._____({\"question\" : \"What is the basic theory of quantum physics?\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66935d1b-9ad9-40f1-ad3a-421d7dfb0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: invoke the chain\n",
    "response = full_chain.invoke({\"_____\" : \"What is Merlion?\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03cc02-2321-4c89-89cb-9418f7f59f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aee89b-76d0-4014-9d44-e84f51af150b",
   "metadata": {},
   "source": [
    "## Runnable Interface\n",
    "This `Runnable` interface provides a standard way to create modular and resuable components in a chain or pipeline. It defines the behavior of components that can process inputs and produce outputs, ensuring interoperability across LangChain's ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b10fa0-88d6-4795-a944-6447ef395820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e944e51-8150-44ed-a7b0-c925240924d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99ae12-bed9-445b-897e-c91dd60745a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert AI tweet generator. \n",
    "Observe the user's prompt, and generate a witty tweet, and include emojis and hashtags.\n",
    "\n",
    "Prompt: {prompt}\n",
    "Tweet: \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1e572-e165-4bfd-abbb-6dca0f9d2381",
   "metadata": {},
   "source": [
    "- A simple `tweet_generator` is composed by chaining together a prompt, model and an output parser in sequence.\n",
    "- Chaining is possible because the components (prompt, model and output parser) implemented the `runnable` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e373f-b9be-4332-b853-70a38b911172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create the chainining: first prompt -> model -> StrOutputParser\n",
    "tweet_generator = _____ | _____ | _____()\n",
    "\n",
    "# TODO: invoke tweet generator chain\n",
    "tweet = tweet_generator._____({\"_____\": \"Langchain releases LangGraph for building stateful, multi-action applications. https://langchain.com\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca2770-c029-4c9d-8c4e-39bcdab57ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the blanks\n",
    "second_prompt = _____._____(\"\"\"\n",
    "You are an expert in AI tweet fixer.\n",
    "Fix user's original tweet and based it on the mood.\n",
    "\n",
    "Original Tweet: {tweet}\n",
    "Mood: {mood}\n",
    "Fixed Tweet:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8034622-1d4a-4cfb-bcd1-334076b2f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_fixer = second_prompt | model | StrOutputParser()\n",
    "fixed_tweet = tweet_fixer.invoke({\"tweet\" : tweet, \"mood\" : \"funny\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bccf70-8aa8-4e3f-a04d-41675603219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5320f78-01ab-4bc4-bc66-0de1142ed7fe",
   "metadata": {},
   "source": [
    "### RunnableParallel\n",
    "Execute more than one `Runnable` components concurrently. It is particularly useful for tasks that can be executed independently, as it improves efficiency by running these tasks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8e933-8e93-4763-be22-f23eb7165a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b981a1-997d-44b1-8a7c-b3b54339ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\") | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff15d8-d664-4dac-b99d-09febf864860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableParallel\n",
    "\n",
    "# TODO: initialise the runnable parallel\n",
    "parallel_chain = _____(joke=chain1, short_poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1aa187-4c18-44a1-b14e-cb317647ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# TODO: invoke the parallel chain\n",
    "response = parallel_chain._____({\"topic\" : \"bears\"})\n",
    "\n",
    "# the time measured is too tiny to tell the difference between parallel or batch chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366dd62-446f-481d-91ab-125d142b27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64271ca4-838f-4d60-8516-4ed01a36349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['joke'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f35d25-7e72-4f72-a099-96a4776297a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['short_poem'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ac731-09db-4b89-8a23-53fde60c92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052ea36-eb3e-4231-9951-9a6d7be1541e",
   "metadata": {},
   "source": [
    "### RunnablePassthrough\n",
    "Serves as a simple utility for passing data through without modification. It is useful in a pipeline or chain of operations when you want to retain certain intermediate results or bypass specific steps without applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f81ea-ba22-418a-9148-ab954332d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2237a-bb74-42ce-9d31-9299dead67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"mood\" and \"prompt\" are passed as input to \"tweet_chain\"\n",
    "# RunnablePassthrough() makes both of these passes through unchanged\n",
    "# itemgetter extracts the value of mood parameter and assigned it to \"mood\"\n",
    "\n",
    "tweet_chain = RunnableParallel(\n",
    "    {\n",
    "        \"mood\": RunnablePassthrough() | itemgetter(\"mood\"),\n",
    "        \"tweet\": tweet_generator\n",
    "    }\n",
    ") | tweet_fixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bb890-07f0-4176-9ae1-807167537117",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_chain.invoke(\n",
    "    {\n",
    "        \"prompt\": \"Langchain releases LangGraph for building stateful, multi-action applications. https://langchain.com\",\n",
    "        \"mood\": \"sarcastic\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2adeb26-f41a-4927-b3f3-b329b82b29eb",
   "metadata": {},
   "source": [
    "### Batch\n",
    "Using this method can significantly improve performance when needing to process multiple independent inputs as the processing can be done in parallel instead of sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e5349-7532-4eba-af68-d1005104f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "response = parallel_chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n",
    "\n",
    "# the time measured is too tiny to tell the difference between parallel or batch chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07290588-e28d-4753-a5a2-db93e78c3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[0]['joke'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf7790-0d95-4b03-bc08-f5c7325a67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[0]['short_poem'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d08d48-10a0-41cf-9053-bc63ee166d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[1]['joke'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9905e0-d0b8-4ada-a36e-4149487dd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[1]['short_poem'].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b73f2b-7177-4405-b4b4-07ae081643f1",
   "metadata": {},
   "source": [
    "### Stream\n",
    "Streaming enhances the responsiveness of application by displaying the output progressively, even before a complete response is ready. Due the latency of LLMs response generation, streaming improves user experience (UX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4153b37-06f7-49da-8b89-ed392049558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_str = \"\"\n",
    "short_poem_str = \"\"\n",
    "\n",
    "for s in parallel_chain.stream({\"topic\" : \"dog\"}):\n",
    "    if 'joke' in s:\n",
    "        joke_str = joke_str + s['joke'].content + \" | \"\n",
    "    elif 'short_poem' in s:\n",
    "        short_poem_str = short_poem_str + s['short_poem'].content + \" | \"\n",
    "\n",
    "print(joke_str)\n",
    "print('-'*10)\n",
    "print(short_poem_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9353a-a869-4d85-8460-ae67010e8fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
