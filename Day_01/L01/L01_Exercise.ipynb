{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da192a6b-8b47-4b15-9086-e61200e0b8c3",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/Day_01/L01/L01_Exercise.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0829e84d-bc72-42e3-b4f3-3451b05ef53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d94109-0dbf-40e1-8c94-2048843cdd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    }
   ],
   "source": [
    "# print module version(s)\n",
    "\n",
    "import nltk\n",
    "print (nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a0edaf-b17d-46da-b17f-cb3daae66961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/nat/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/nat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/nat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download and install the resource \n",
    "# https://www.nltk.org/api/nltk.tokenize.punkt.html\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44830672-4a2d-4a0d-88a6-6e37e2050239",
   "metadata": {},
   "source": [
    "# Lesson 01 Exercise\n",
    "\n",
    "Given the text below, what are the preprocessing techniques you could apply?\n",
    "\n",
    "\"Generative AI is revolutionizing many industries by automating creative tasks such as writing, designing, and even composing music. These models, like GPT and DALL·E, learn from vast amounts of data to produce original content that mimics human output. However, the ethical implications of this technology, including concerns about bias and intellectual property, are raising important debates. As generative AI continues to evolve, it is crucial for researchers and developers to address these challenges to ensure responsible and fair use.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49605a-99d0-45a3-9f29-9c55ec858707",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Generative AI is revolutionizing many industries by automating creative tasks such as writing, designing, and even composing music. \n",
    "These models, like GPT and DALL·E, learn from vast amounts of data to produce original content that mimics human output. \n",
    "However, the ethical implications of this technology, including concerns about bias and intellectual property, are raising important debates. \n",
    "As generative AI continues to evolve, it is crucial for researchers and developers to address these challenges to ensure responsible and fair use.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88153aac-c98c-40f1-a2ad-5bf539cdc8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint, short for \"pretty-print,\" is a Python module and function that helps print data structures \n",
    "# (like lists, dictionaries, etc.) in a more readable and formatted way\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2aca10-6ef3-411d-b7e8-c629a9f0ce8f",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of splitting the text into individual words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf685f8-4b24-4c8d-901c-c4d445a895a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the blanks\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(text)  # Word-level tokenization\n",
    "sentence_tokens = sent_tokenize(text)  # Sentence-level tokenization\n",
    "\n",
    "pprint(word_tokens)\n",
    "print()\n",
    "pprint(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b1590-0cc2-4188-a07e-9fda0f806fac",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging involves assigning a part of speech (noun, verb, adjective, etc.) to each word in the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e4f08-2c5f-4289-a01c-b122b89863ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the blank\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags = pos_tag(word_tokens)  # Apply POS tagging to word tokens\n",
    "\n",
    "pprint(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb7913-c82d-4142-bc01-c7187ce56137",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming reduces words to their root form by removing suffixes (e.g., \"revolutionizing\" becomes \"revolution\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b0e16-15ce-4884-b2c0-05d44989d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# TODO: initialise the porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in word_tokens]\n",
    "\n",
    "pprint(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d91c36-8144-4bb6-9ba2-4e24fbbc0212",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base or dictionary form (lemma), but considers the context (e.g., \"better\" becomes \"good\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536431b-948a-440c-9ccf-f621b47d6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in word_tokens]  # 'v' is for verb\n",
    "\n",
    "pprint(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5acc3-c017-404f-9515-8d17d03fe9b7",
   "metadata": {},
   "source": [
    "## Stop Words Removals\n",
    "\n",
    "Stop words are common words like \"is\", \"the\", and \"and\" that are often removed because they carry little meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e98da-410a-48c0-b757-93396b7d348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# TODO: the English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "pprint(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d6408-9ebc-4094-bc53-3f985119e80f",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Chunking groups words into phrases based on POS tags, like noun phrases (NP) or verb phrases (VP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac2534-2e7f-407f-aa93-e33b1903836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# Define multi-word expressions (phrases you want to treat as single tokens)\n",
    "mwe = [('Generative', 'AI'), ('generative', 'AI')]\n",
    "\n",
    "# Initialize the MWE tokenizer\n",
    "tokenizer = MWETokenizer(mwe)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text.split())\n",
    "\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af409790-c89c-4a36-a5eb-093f268e5ec7",
   "metadata": {},
   "source": [
    "## Summary of Steps:\n",
    "\n",
    "1. Tokenize the paragraph into words or sentences.\n",
    "2. Apply POS tagging to the word tokens to label each word with its part of speech.\n",
    "3. Use Stemming to reduce each word to its root form.\n",
    "4. Use Lemmatization to convert each word to its base form, considering context.\n",
    "5. Remove stop words to filter out common words that don’t add much meaning.\n",
    "6. Perform Chunking to group tokens into syntactic phrases, like noun phrases or verb phrases.\n",
    "\n",
    "These techniques will help you explore different aspects of text preprocessing and syntactic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1132ba2-3e89-405a-8fcf-112a62c54484",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer, WordNetLemmatizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m----> 7\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m word_tokenize(\u001b[43mtext\u001b[49m)  \u001b[38;5;66;03m# Word-level tokenization\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sentence_tokens \u001b[38;5;241m=\u001b[39m sent_tokenize(text)  \u001b[38;5;66;03m# Sentence-level tokenization\u001b[39;00m\n\u001b[1;32m     10\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m pos_tag(word_tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, MWETokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "txt = '''\n",
    "\n",
    "'''\n",
    "\n",
    "word_tokens = word_tokenize(txt)\n",
    "\n",
    "pos_tags = pos_tag(word_tokens)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in word_tokens]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in word_tokens]  # 'v' is for verb\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "mwe = [('Generative', 'AI'), ('generative', 'AI')]\n",
    "\n",
    "tokenizer = MWETokenizer(mwe)\n",
    "\n",
    "tokens = tokenizer.tokenize(text.split())\n",
    "\n",
    "pprint(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
