{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/Day_02/L11/L11_Answer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `SST_DP2025`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f4c2-8506-415c-a295-041ee40d9eda",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab.\n",
    "\n",
    "## <font color=\"#FF0000\">IMPORTANT</font>\n",
    "If you are running this code in Google Colab, you need to run the below commands to download `MicrosoftEULA.txt` to Google Colab.\n",
    "\n",
    "Comment out the below code with '#' if you are not running it in your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d130a9-af39-4ea1-ad54-a2c9b87cce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/koayst-rplesson/SST_DP2025/refs/heads/main/Day_02/L11/MicrosoftEULA.txt\n",
    "!dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a344c6-9289-4351-9677-8e7f778c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langgraph\n",
    "%pip install --quiet -U langchain-openai\n",
    "%pip install --quiet -U grandalf\n",
    "%pip install --quiet -U langchain-community\n",
    "%pip install --quiet -U faiss-cpu\n",
    "%pip install --quiet -U pytube\n",
    "%pip install --quiet -U youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933f2767-9f87-48e8-b1b7-c0b952d2cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grandalf               0.8\n",
    "# langchain              0.3.11\n",
    "# langgraph              0.2.59\n",
    "# langchain-core         0.3.24\n",
    "# langchain-openai       0.2.12\n",
    "# langchain-community    0.3.12\n",
    "# openai                 1.57.2\n",
    "# pydantic               2.10.3\n",
    "# pytube                 15.0.0\n",
    "# faiss-cpu              1.9.0.post1\n",
    "# youtube-transcript-api 0.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7281bdb-a238-4cad-abac-6679aa169ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b627ca3-d583-4067-841f-7b0eb98a1dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "# setup the Langchain API Key\n",
    "# Goto https://smith.langchain.com/ to register and get the key\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] =\"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc02a79-8283-4235-b570-c813d51bff91",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Retrieve relevant information from an external data source and pass the data to LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f7bf4-560b-4db7-8b8f-30bd18d42da7",
   "metadata": {},
   "source": [
    "### Load a text file\n",
    "- Load a text file using `TextLoader`.\n",
    "- Chunk the document using `CharacterTextSplitter`.\n",
    "- Encode the document using `OpenAIEmbeddings` and store it in a vector store.\n",
    "- Use `create_stuff_documents_chain` to create a chain for passing a list of Documents to a model.\n",
    "- `create_retrieval_chain` creates retrieval chain to retrieves documents.\n",
    "- `FAISS` is an open-source library designed for efficient similarity search and clustering of dense vectors.\n",
    "- [Langchain Hub](https://blog.langchain.dev/langchain-prompt-hub/) is a home for uploading, browsing, pulling and managing prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892ce11d-96fc-4aad-b307-0ec50122e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c9026c-14e7-4aff-a66e-8ffd748b61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_loader = TextLoader(\n",
    "    \"MicrosoftEULA.txt\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "documents = document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ad4722-3374-4d03-b409-77c69dd7875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1406, which is longer than the specified 1000\n",
      "Created a chunk of size 1161, which is longer than the specified 1000\n",
      "Created a chunk of size 1083, which is longer than the specified 1000\n",
      "Created a chunk of size 1125, which is longer than the specified 1000\n",
      "Created a chunk of size 1073, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# the chunk_size (1000) is arbitary. May need to experiement to find the optimum size\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "text = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d79410-605a-4f95-8906-f051b2fd3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.from_documents(text, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e281020-5738-49f1-a8fb-1e1a65a7e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "# You will get a warning if you haven't not gotten the Langchain API key\n",
    "# LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5f7ad3-2e78-4565-87b4-42fbfc2e6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'input'] optional_variables=['chat_history'] input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001A7B7D99790>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'chat_history': []} metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(retrieval_qa_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d98e9c-4651-4267-9a8d-54991e9f3d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is ok to just use Chapt-3.5-turbo for this sample code\n",
    "\n",
    "model.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a7ffc8-109e-4dd3-9f0f-5d1155c8d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(model, retrieval_qa_chat_prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731b8d02-4f63-4135-b3f6-e7c66e876526",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\n",
    "    \"input\":\"What is this document about? Summarize it in a paragraph\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a936c08-616a-4f87-8be7-14d4e911ce84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This document is a Microsoft Software License Agreement for the Windows Operating System. It outlines the rights and conditions for using the Windows software, including the requirement to review the entire agreement and any supplemental license terms that accompany the software. By accepting the agreement or using the software, users consent to the transmission of certain information during activation and usage as per the privacy statement. It specifies that depending on how the software was obtained, the agreement is between the user and the device manufacturer, software installer, or Microsoft Corporation. The agreement emphasizes the importance of reviewing all terms, including any linked terms, before using the software or services. It also provides instructions on how to access and review the terms during software usage.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f64bb8-f2c0-45a0-bcaa-8ee393803f46",
   "metadata": {},
   "source": [
    "### Retrieval and Query a YouTube Video\n",
    "\n",
    "Observe in this sample code, the only change is the YouTube video loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25db38c8-4bab-405d-b0f8-790a1169a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94fa5a9d-71ea-4b89-bb14-8b69d094cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube: How a Worm Could Save Humanity From Bad AI | Ramin Hasani | TED\n",
    "# https://www.youtube.com/watch?v=x6oM9hQMjUY\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url( \n",
    "    \"https://www.youtube.com/watch?v=x6oM9hQMjUY\",\n",
    "    add_video_info=False,\n",
    "    language=[\"en\"]    \n",
    ")\n",
    "\n",
    "youtube_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba1b8f63-3c3e-4464-b4c6-bd5b75b950c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'x6oM9hQMjUY'}, page_content=\"My wildest dream is to design\\nartificial intelligence that is our friend, you know. If you have an AI system\\nthat helps us understand mathematics, you can solve the economy of the world. If you have an AI system\\nthat can understand humanitarian sciences, we can actually solve\\nall of our conflicts. I want this system to, given Einstein’s\\nand Maxwell’s equations, take it and solve new physics, you know. If you understand physics,\\nyou can solve the energy problem. So you can actually design ways for humans to be the better versions of themselves. I'm Ramin Hasani. I’m the cofounder and CEO of Liquid AI. Liquid AI is an AI company built\\non top of a technology that I invented back at MIT. It’s called “liquid neural networks.” These are a form of flexible intelligence, as opposed to today's AI systems\\nthat are fixed, basically. So think about your brain. You can change your thoughts. When somebody talks to you, you can completely change\\nthe way you respond. You always have a mechanism\\nthat we call feedback in your system. So basically when you receive information\\nfrom someone as an input, you basically process that information,\\nand then you reply. For liquid neural networks, we simply got those feedback mechanisms,\\nand we added that to the system. So that means it has\\nthe ability of thinking. That property is inspired by nature. We looked into brains of animals\\nand, in particular, a very, very tiny worm\\ncalled “C. elegans” The fascinating fact\\nabout the brain of the worm is that it shares 75 percent\\nof the genome that it has with humans. We have the entire genome mapped. So we understand a whole lot about the functionality\\nof its nervous system as well. So if you understand the properties\\nof cells in the worm, maybe we can build intelligent systems\\nthat are as good as the worm and then evolve them into systems\\nthat are better than even humans. The reason why we are studying nature\\nis the fact that we can actually, having a shortcut through exploring\\nall the possible kind of algorithms that you can design. You can look into nature, that would give you like, a shortcut to really faster get\\ninto efficient algorithms because nature has done a lot of search, billions of years of evolution, right? So we learned so much\\nfrom those principles. I just brought a tiny\\nprinciple from the worm into artificial neural networks. And now they are flexible, and they can solve problems\\nin an explainable way that was not possible before. AI is becoming very capable, right? The reason why AI is hard to regulate is because we cannot understand,\\neven the people who design the systems, we don't understand those systems. They are black boxes. With Liquid, because we are\\nfundamentally using mathematics that are understandable, we have tools to really understand and pinpoint which part of the system\\nis responsible for what, you're designing white box systems. So if you have systems\\nthat you can understand their behavior, that means even if you scale them\\ninto something very, very intelligent, you can always have a lot\\nof control over that system because you understand it. You can never let it go rogue. So all of the crises\\nwe are dealing with right now, you know, doomsday kind of scenarios, is all about scaling a technology\\nthat we don't understand. With Liquid, our purpose\\nis to really calm people down and show people that, hey, you can have very powerful systems, that you have a lot\\nof control and visibility into their working mechanisms. The gift of having something\\n[with] superintelligence is massive, and it can enable a lot of things for us. But at the same time, we need to have control\\nover that technology. Because this is the first time\\nthat we’re going to have a technology that is going to be better\\nthan all of humanity combined.\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e879307-5461-4718-8e21-fcc3c9856c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "text = text_splitter.split_documents(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0f39f8f-c8b4-4c2b-977d-02e9fe031ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.from_documents(text, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69ba2533-ff14-4660-8157-274c4ec77c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "448ae8aa-b647-453c-bbb5-008900a50b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(model, retrieval_qa_chat_prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a27897cb-df8a-43e1-b12d-c5afbb596ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramin Hasani, the CEO of Liquid AI, envisions designing artificial intelligence that can be a beneficial friend to humanity. By developing liquid neural networks inspired by the brain of the C. elegans worm, his company aims to create flexible and understandable AI systems that can solve complex problems and evolve beyond human capabilities. By understanding the behavior of the AI through transparent mathematics, Liquid AI seeks to provide a level of control over superintelligent technology, ensuring it does not pose a threat to humanity. Their goal is to demonstrate that powerful AI can coexist safely with humans, offering immense benefits while maintaining oversight and preventing potential doomsday scenarios caused by unchecked technology advancement.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"input\" : \"Summarize it in a paragraph\"}\n",
    ")\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e557d997-baeb-4fac-b992-b3df02028b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fascinating fact about the brain of the worm is that it shares 75 percent of its genome with humans.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"input\" : \"What is the fascinating fact about the brain of the worm?\"}\n",
    ")\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f1890-5ae3-4804-bc00-7b89d0b43958",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "In LangChain, the evaluation of Language Models (LLMs) involves multiple methods such as comparing chain outputs, pairwise string comparisons, string distances, and embedding distances. These evaluations help determine the most preferred model by analyzing the differences in their outputs.\n",
    "\n",
    "The types of evaluator are documented [here](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.schema.EvaluatorType.html#langchain.evaluation.schema.EvaluatorType)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ace3e7de-3bf4-46d4-b673-25bb05640a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc6cf97-7b51-4146-9f7e-fb5929932c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_evaluator in module langchain.evaluation.loading:\n",
      "\n",
      "load_evaluator(evaluator: langchain.evaluation.schema.EvaluatorType, *, llm: Optional[langchain_core.language_models.base.BaseLanguageModel] = None, **kwargs: Any) -> Union[langchain.chains.base.Chain, langchain.evaluation.schema.StringEvaluator]\n",
      "    Load the requested evaluation chain specified by a string.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    evaluator : EvaluatorType\n",
      "        The type of evaluator to load.\n",
      "    llm : BaseLanguageModel, optional\n",
      "        The language model to use for evaluation, by default None\n",
      "    **kwargs : Any\n",
      "        Additional keyword arguments to pass to the evaluator.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Chain\n",
      "        The loaded evaluation chain.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from langchain.evaluation import load_evaluator, EvaluatorType\n",
      "    >>> evaluator = load_evaluator(EvaluatorType.QA)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_evaluator)\n",
    "\n",
    "# notice that `llm` is optional\n",
    "# according to the documentation: The language model to use for evaluation, if none is provided, a default ChatOpenAI gpt-4 model will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0df1b-7a3b-482a-a926-bd310f67d5f1",
   "metadata": {},
   "source": [
    "### Pairwise String Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02db89fd-f1ac-46c0-abbe-1bb01f9c544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before you run the code, don't forget to setup the OpenAI API key\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90709bbd-b5ce-43de-928c-6a5813e695d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_01 = '''\n",
    "LangChain is a Python library designed to make it easier to build applications with large language models, \n",
    "providing tools for chaining components and managing complex natural language processing workflows\n",
    "'''\n",
    "\n",
    "text_02 = '''\n",
    "LangChain is a Python framework that connects learners and tutors worldwide, \n",
    "offering personalized lessons, real-time practice, and transparent payment using blockchain technology \n",
    "to ensure security and fairness.\n",
    "'''\n",
    "\n",
    "# some criteria require reference labels to work correctly\n",
    "reference = '''\n",
    "LangChain is a Python framework designed to streamline AI application development, focusing on real-time\n",
    "data processing and integration with Large Language Models.\n",
    "'''\n",
    "\n",
    "eval_result = evaluator.evaluate_string_pairs(\n",
    "    prediction = text_01,\n",
    "    prediction_b = text_02,\n",
    "    input = \"describe LangChain in thirty words\",\n",
    "    reference = reference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4418f947-9ae6-4eda-97f6-f1a5e1bdbc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant A's response is more accurate and relevant to the user's question. It correctly describes LangChain as a Python library designed for building applications with large language models and managing complex natural language processing workflows. On the other hand, Assistant B's response is incorrect as it describes LangChain as a platform that connects learners and tutors worldwide, which is not accurate. Therefore, Assistant A's response is more helpful, correct, and demonstrates a better depth of thought. \n",
      "\n",
      "Final Verdict: [[A]]\n"
     ]
    }
   ],
   "source": [
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea20f8-3720-4475-b780-c4650cbb8ba4",
   "metadata": {},
   "source": [
    "### Predefined Criteria - Conciseness\n",
    "According to `Cambridge Dictionary`, conciseness is the quality of being short and clear, and expressing what needs to be said without unnecessary words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7473574-6f55-4e89-b2bb-b58222aae751",
   "metadata": {},
   "source": [
    "### Concise Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2531607a-bb88-4430-812a-74fa8aff08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"criteria\", criteria = \"conciseness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71a31fcf-27ba-417e-a53b-53043edec64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "concise='''\n",
    "Generative AI is a type of artificial intelligence that can autonomously create new and original content, such as images, \n",
    "text, or other forms of data, using algorithms and models. It has the ability to generate outputs that mimic human-created \n",
    "content without relying solely on predefined patterns.\n",
    "'''\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction = concise,\n",
    "    input = \"What is generative AI?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34fc1a7b-2b6d-4a40-995d-e493e2991eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The criterion for this assessment is conciseness. \n",
      "\n",
      "The submission is a brief explanation of generative AI, providing a clear definition and an example of what it can do. It does not include unnecessary information or go off-topic. \n",
      "\n",
      "The submission is concise and to the point, therefore it meets the criterion.\n",
      "\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81631aa6-3bb6-4112-8f44-60ef46cc062d",
   "metadata": {},
   "source": [
    "### Inconcise Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86aede8c-01ec-426e-b16c-64e2b8dea6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "inconcise='''\n",
    "In the vast landscape of artificial intelligence, generative AI emerges as a subset intricately enmeshed in a labyrinthine \n",
    "array of algorithms, particularly those rooted in the complex neural networks exemplified by the captivating Generative \n",
    "Adversarial Networks (GANs). This convoluted field empowers machines to autonomously and creatively navigate the expansive \n",
    "spectrum of content creation, spanning from vividly evocative imagery to the nuanced articulation found in various textual expressions. \n",
    "This intricate process, laden with multifaceted intricacies, tangentially mirrors the profound subtleties inherent in the intricate \n",
    "tapestry of human cognition and expressive exploration.\n",
    "'''\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction = inconcise,\n",
    "    input = \"What is generative AI?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bd1ee6d-236c-4e37-8c35-3e56755a10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The criterion for this assessment is conciseness, which means the submission should be brief, to the point, and without unnecessary details or complex language.\n",
      "\n",
      "Looking at the submission, it is clear that the answer is not concise. The language used is complex and verbose, with many unnecessary details and metaphors. The answer could have been much shorter and simpler, while still conveying the same information.\n",
      "\n",
      "For example, the first sentence could have been simplified to: \"Generative AI is a subset of artificial intelligence that uses algorithms like Generative Adversarial Networks (GANs).\" This would have conveyed the same information in a much more concise manner.\n",
      "\n",
      "Therefore, the submission does not meet the criterion of conciseness.\n",
      "\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682243ee-8941-4d61-a11d-0861d14a0e7c",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "According to `Cambridge Dictionary`, correctness is the quality of being in agreement with the true facts or with what is generally accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "240ac969-6ec6-4a46-94ed-785e21c44a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"labeled_criteria\", criteria = \"correctness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69c7190c-f2f2-4dc8-b12a-aa19a9c9d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopi C Kosong\n",
    "# Black coffee with evaporated milk and no sugar – think of it as a cafe au lait\n",
    "# Ref: https://thehoneycombers.com/singapore/order-kopi-singapore/\n",
    "\n",
    "correctness_test_1='''\n",
    "The name 'Kopi C Kosong' means 'empty coffee' which refers to the fact that no milk is added to the coffee.\n",
    "'''\n",
    "\n",
    "correctness_test_2='''\n",
    "'Kopi C Kosong' is a coffee with evaporated milk but without any sugar.\n",
    "'''\n",
    "\n",
    "reference='''\n",
    "'Kopi C Kosong' is a coffee with evaporated milk but without any sugar. It is one of the variations in a \n",
    "wide array of coffee styles available in the coffee shops (kopitiams) of Singapore and Malaysia.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e302d8f2-dc9a-4395-8068-469cbe0300bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input = \"What is Kopi C Kosong?\",\n",
    "    prediction = correctness_test_1,\n",
    "    reference = reference,\n",
    ")\n",
    "\n",
    "print(f'With ground truth: {eval_result[\"score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e57ce91a-b7ea-4721-b7e3-4a8c3298077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With ground truth: 1\n"
     ]
    }
   ],
   "source": [
    "# test 2\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input = \"What is Kopi C Kosong?\",\n",
    "    prediction = correctness_test_2,\n",
    "    reference = reference,\n",
    ")\n",
    "\n",
    "print(f'With ground truth: {eval_result[\"score\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd360d-7e0c-4257-9e12-c0fb855ca57a",
   "metadata": {},
   "source": [
    "## Custom Criteria\n",
    "- LangChain supports custom criteria and predefined principles for evaluation\n",
    "- Custom criteria can be defined using a key-value pairs {criterion_name : criterion_description}\n",
    "- These criteria can be used to assess outputs based on requirements or rubrics\n",
    "\n",
    "**Note**\n",
    "[LangChain](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/string/criteria_eval_chain/#custom-criteria): it's recommended that you create a single evaluator per criterion. This way, separate feedback can be provided for each aspect. Additionally, if you provide antagonistic criteria, the evaluator won't be very useful, as it will be configured to predict compliance for ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82d83452-5799-4fcf-b9f7-dd63e708f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_criteria = {\n",
    "    \"simplicity\": \"Is the language straightforward and unpretentious?\",\n",
    "    \"clarity\": \"Are the sentences clear and easy to understand?\",\n",
    "    \"precision\": \"Is the writing precise, with no unnecessary words or details?\",\n",
    "    \"truthfulness\": \"Does the writing feel honest and sincere?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7242d6e-9478-486f-b423-fc2c316bfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"pairwise_string\", criteria=custom_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78c597fa-7405-4400-a030-dd2ffc94b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Every cheerful household shares a similar rhythm of joy; but sorrow, in each household, plays a unique, haunting melody.\",\n",
    "    prediction_b=\"Where one finds a symphony of joy, every domicile of happiness resounds in harmonious,\"\n",
    "                 \"identical notes; yet, every abode of despair conducts a dissonant orchestra, each \"\n",
    "                 \"playing an elegy of grief that is peculiar and profound to its own existence.\",\n",
    "    input=\"Write some prose about families.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dd4ac6f-2532-40da-941a-f3753807707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant A's response is simpler, clearer, and more precise than Assistant B's. Assistant A uses straightforward language and clear sentences, making it easy to understand. On the other hand, Assistant B's response is more complex and uses more sophisticated language, which may make it harder for some users to understand. Both responses seem truthful and sincere, but Assistant A's response is more in line with the criteria provided. Therefore, Assistant A's response is better. \n",
      "\n",
      "Final Verdict: [[A]]\n"
     ]
    }
   ],
   "source": [
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744cb85-be5d-415e-8a01-f75dd60e3bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
