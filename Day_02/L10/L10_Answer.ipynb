{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/Day_02/L10/L10_Answer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `SST_DP2025`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f4c2-8506-415c-a295-041ee40d9eda",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a344c6-9289-4351-9677-8e7f778c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langgraph\n",
    "%pip install --quiet -U langchain-openai\n",
    "%pip install --quiet -U grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f2767-9f87-48e8-b1b7-c0b952d2cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grandalf         0.8\n",
    "# langchain        0.3.11\n",
    "# langgraph        0.2.59\n",
    "# langchain-core   0.3.24\n",
    "# langchain-openai 0.2.12\n",
    "# openai           1.57.2\n",
    "# pydantic         2.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7281bdb-a238-4cad-abac-6679aa169ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc02a79-8283-4235-b570-c813d51bff91",
   "metadata": {},
   "source": [
    "## Chains\n",
    "\n",
    "### Basic/Entry Chain\n",
    "A basic chain or entry chain using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892ce11d-96fc-4aad-b307-0ec50122e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c9026c-14e7-4aff-a66e-8ffd748b61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"How do I say 'Hello' in {language}?\",\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2fe6ff-5fba-46d9-a05d-ec835ce93b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d42c119-53d9-4414-9b67-5e3b4bc8cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "print(model.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a7ffc8-109e-4dd3-9f0f-5d1155c8d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71a9775-1214-49a1-aa80-9c47dea06de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You say 'Hello' in French as \"Bonjour.\"\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"language\" : \"French\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf73b80-34d2-40b6-91bc-3ef985f57750",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain (Single Input / Single Output)\n",
    "Simple chain where the output of one step feed directly into next. A streamlined version of a sequential chain, where the output of each step directly becomes the input for the next. It is perfect for straightforward workflows without the need for advanced memory or branching logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a819207-b750-42b5-8728-d1faa7a8de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95115df6-1c41-481d-af25-7c5ca0c3b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5720a98b-6f4c-4572-b3f8-71bba8ad324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is a good name for a company that specialises in making {product}?\"\n",
    ")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a short introduction of company: {company} .\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97035990-d579-43fd-82de-968242d54ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    first_prompt\n",
    "    | model\n",
    "    | {'company' : RunnablePassthrough()}   # 'company' is generated after the first prompt\n",
    "    | second_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ").with_types(input_type=Dict[str, str], output_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "335cc210-fd6d-4595-a5f9-46fb22d01e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Welcome to StridePro** – where every step counts! At StridePro, we specialize in high-performance running shoes designed to enhance your running experience. Our mission is to empower runners of all levels, from beginners to seasoned marathoners, with innovative footwear that combines comfort, durability, and cutting-edge technology. With a commitment to quality and style, our shoes are crafted to support your journey, whether you're hitting the pavement or conquering trails. Join us in celebrating the joy of running and discover your perfect stride with StridePro!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'product' : 'Running shoe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e842eb7-ead1-406c-b0d1-9e14a84caa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | ChatPromptTemplate |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "     +-------------+       \n",
      "     | Passthrough |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | ChatPromptTemplate |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "# display the chain as a graph\n",
    "\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31752660-b372-43ce-a40d-92fe8b7c399b",
   "metadata": {},
   "source": [
    "### Sequential Chain\n",
    "The `Sequential Chain` in LangChain is a simple way to execute a series of tasks in order, where the output of one step becomes the input for the next. This type of chain is ideal for workflows where the steps depend on one another in a linear sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae89651e-78e4-419a-ba8c-6d8f94f6e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d0af63-4534-4c44-aa46-e3119918f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a brief introduction about {topic}.\"\n",
    ")\n",
    "\n",
    "first_chain = first_prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1a4d835-98ee-4118-95af-f63bdaaae514",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"From this introduction: \\\"{introduction}\\\", generate three key bullet points.\"\n",
    ")\n",
    "\n",
    "second_chain = second_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c28d93-c578-4ad2-852e-fe7f2bc16748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe in this prompt, it takes in two inputs 'introduction' and 'bullet_points' from earlier chains\n",
    "\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Combine the introduction: \\\"{introduction}\\\" with the key points: \\\"{bullet_points}\\\".\\n\"\n",
    "    \"Write a concise summary from this information.\"\n",
    ")\n",
    "\n",
    "third_chain = third_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdc07d86-a2c0-4c1b-8355-d3c6f165198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"introduction\" : first_chain}\n",
    "    | RunnablePassthrough.assign(bullet_points=second_chain)\n",
    "    | RunnablePassthrough.assign(summary=third_chain)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d921243-c1a5-4d8e-bb60-4fa28fc40c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"topic\" : \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24e70831-a7fb-466a-8a6f-f3bb65faefb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary of Artificial Intelligence (AI)**\n",
      "\n",
      "Artificial Intelligence (AI) involves the simulation of human intelligence processes by machines, including learning, reasoning, problem-solving, perception, and language understanding. AI technologies, such as machine learning, neural networks, and natural language processing, are increasingly integrated into various sectors like healthcare, finance, transportation, and entertainment, significantly enhancing efficiency and innovation. However, the evolution of AI also raises important ethical and societal questions about its impact on jobs, privacy, and decision-making, highlighting the need for careful consideration of its implications.\n"
     ]
    }
   ],
   "source": [
    "print(response['summary'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1653ff4-993b-4dee-9733-222a48f1ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +-----------------------------+           \n",
      "              | Parallel<introduction>Input |           \n",
      "              +-----------------------------+           \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                  +--------------------+                \n",
      "                  | ChatPromptTemplate |                \n",
      "                  +--------------------+                \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                      +------------+                    \n",
      "                      | ChatOpenAI |                    \n",
      "                      +------------+                    \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "             +------------------------------+           \n",
      "             | Parallel<bullet_points>Input |           \n",
      "             +------------------------------+           \n",
      "                   ***               ***                \n",
      "                ***                     ***             \n",
      "              **                           ***          \n",
      "+--------------------+                        **        \n",
      "| ChatPromptTemplate |                         *        \n",
      "+--------------------+                         *        \n",
      "           *                                   *        \n",
      "           *                                   *        \n",
      "           *                                   *        \n",
      "    +------------+                      +-------------+ \n",
      "    | ChatOpenAI |                      | Passthrough | \n",
      "    +------------+*                     +-------------+ \n",
      "                   ***               ***                \n",
      "                      ***         ***                   \n",
      "                         **     **                      \n",
      "            +-------------------------------+           \n",
      "            | Parallel<bullet_points>Output |           \n",
      "            +-------------------------------+           \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                +------------------------+              \n",
      "                | Parallel<summary>Input |              \n",
      "                +------------------------+              \n",
      "                   ***               ***                \n",
      "                ***                     ***             \n",
      "              **                           ***          \n",
      "+--------------------+                        **        \n",
      "| ChatPromptTemplate |                         *        \n",
      "+--------------------+                         *        \n",
      "           *                                   *        \n",
      "           *                                   *        \n",
      "           *                                   *        \n",
      "    +------------+                      +-------------+ \n",
      "    | ChatOpenAI |                      | Passthrough | \n",
      "    +------------+*                     +-------------+ \n",
      "                   ***               ***                \n",
      "                      ***         ***                   \n",
      "                         **     **                      \n",
      "                +-------------------------+             \n",
      "                | Parallel<summary>Output |             \n",
      "                +-------------------------+             \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27cfb617-f591-41d8-9f99-dde7ef7133ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Write a brief introduction about {topic}.'), additional_kwargs={})]),\n",
       " ChatPromptTemplate(input_variables=['introduction'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['introduction'], input_types={}, partial_variables={}, template='From this introduction: \"{introduction}\", generate three key bullet points.'), additional_kwargs={})]),\n",
       " ChatPromptTemplate(input_variables=['bullet_points', 'introduction'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['bullet_points', 'introduction'], input_types={}, partial_variables={}, template='Combine the introduction: \"{introduction}\" with the key points: \"{bullet_points}\".\\nWrite a concise summary from this information.'), additional_kwargs={})])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting Runnables\n",
    "\n",
    "chain.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe0907-1b34-4225-ace0-6a917416bf29",
   "metadata": {},
   "source": [
    "### Routing between Sub-Chains\n",
    "This can be another form of router chain implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f6f79cf-802a-4630-a352-2f6f249f4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cab798f5-29d9-4c10-9db3-6a8471eef6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"Given the user question below, classify it as either being about `Math`, `Science`, or `General`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cd6adbf-abcb-49fb-88a7-fa6ce58cbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa24b781-eca5-4872-a0c4-a8f539785219",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7dc4164-ce0c-45b7-8d67-1db3d8cdbde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Science'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test and ensure the classification is working\n",
    "\n",
    "chain.invoke({\"question\" : \"What is the basic theory of quantum physics?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87acda92-1b6c-45d8-a573-df7e33aa0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a math chain\n",
    "\n",
    "math_chain = PromptTemplate.from_template(\n",
    "\"\"\"You are an expert in Mathematics.\n",
    "Always answer questions starting with \"As my MATH teacher told me\".\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "399d34e4-6485-47fb-b7f2-f598705dff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a science chain\n",
    "\n",
    "science_chain = PromptTemplate.from_template(\n",
    "\"\"\"You are an expert in Science.\n",
    "Always answer questions starting with \"As my SCIENCE teacher told me\".\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b1b9cf0-f18d-4f2a-a262-edbf1c5bedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generic chain\n",
    "\n",
    "general_chain = PromptTemplate.from_template(\n",
    "    \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e62377ed-9269-464e-a073-84288cdf3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom function to route between different outputs\n",
    "\n",
    "def route(info):\n",
    "    if \"math\" in info['topic'].lower():\n",
    "        return math_chain\n",
    "    elif \"science\" in info['topic'].lower():\n",
    "        return science_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b3fa1e0-4ab4-455a-a2c2-367ee324d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the full chain\n",
    "\n",
    "full_chain = (\n",
    "    {\"topic\" : chain, \"question\" : lambda x: x['question']} \n",
    "    | RunnableLambda (route)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bff908df-8c28-48e0-ac65-6891ff90be9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As my SCIENCE teacher told me, the basic theory of quantum physics revolves around the idea that energy and matter exist in discrete units, or \"quanta.\" This theory fundamentally challenges classical physics by introducing concepts such as wave-particle duality, where particles like electrons can exhibit properties of both particles and waves. Additionally, quantum physics emphasizes the role of probability and uncertainty, encapsulated in Heisenberg's Uncertainty Principle, which states that certain pairs of physical properties, like position and momentum, cannot be simultaneously known to arbitrary precision. Overall, quantum physics provides a framework for understanding the behavior of matter and energy at the smallest scales, leading to revolutionary advancements in technology and our understanding of the universe.\n"
     ]
    }
   ],
   "source": [
    "response = full_chain.invoke({\"question\" : \"What is the basic theory of quantum physics?\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66935d1b-9ad9-40f1-ad3a-421d7dfb0434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Merlion is a famous national symbol and landmark of Singapore. It is a mythical creature with the head of a lion and the body of a fish. The lion's head represents Singapore's original name, Singapura, which means \"Lion City\" in Malay, while the fish body symbolizes Singapore's origins as a fishing village. The Merlion statue stands at 8.6 meters (28 feet) tall and is often depicted spouting water from its mouth into Marina Bay. It has become an iconic representation of Singapore, attracting millions of tourists and serving as a popular photo opportunity. The Merlion has various replicas and is associated with the country's identity and tourism.\n"
     ]
    }
   ],
   "source": [
    "response = full_chain.invoke({\"question\" : \"What is Merlion?\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a03cc02-2321-4c89-89cb-9418f7f59f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Given the user question below, classify it as either being about `Math`, `Science`, or `General`.\\n\\nDo not respond with more than one word.\\n\\n<question>\\n{question}\\n</question>\\n\\nClassification:'),\n",
       " PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='You are an expert in Mathematics.\\nAlways answer questions starting with \"As my MATH teacher told me\".\\nRespond to the following question:\\n\\nQuestion: {question}\\nAnswer:'),\n",
       " PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='You are an expert in Science.\\nAlways answer questions starting with \"As my SCIENCE teacher told me\".\\nRespond to the following question:\\n\\nQuestion: {question}\\nAnswer:'),\n",
       " PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Respond to the following question:\\n\\nQuestion: {question}\\nAnswer:')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aee89b-76d0-4014-9d44-e84f51af150b",
   "metadata": {},
   "source": [
    "## Runnable Interface\n",
    "This `Runnable` interface provides a standard way to create modular and resuable components in a chain or pipeline. It defines the behavior of components that can process inputs and produce outputs, ensuring interoperability across LangChain's ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53b10fa0-88d6-4795-a944-6447ef395820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e944e51-8150-44ed-a7b0-c925240924d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a99ae12-bed9-445b-897e-c91dd60745a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert AI tweet generator. \n",
    "Observe the user's prompt, and generate a witty tweet, and include emojis and hashtags.\n",
    "\n",
    "Prompt: {prompt}\n",
    "Tweet: \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1e572-e165-4bfd-abbb-6dca0f9d2381",
   "metadata": {},
   "source": [
    "- A simple `tweet_generator` is composed by chaining together a prompt, model and an output parser in sequence.\n",
    "- Chaining is possible because the components (prompt, model and output parser) implemented the `runnable` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d3e373f-b9be-4332-b853-70a38b911172",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_generator = first_prompt | model | StrOutputParser()\n",
    "tweet = tweet_generator.invoke({\"prompt\": \"Langchain releases LangGraph for building stateful, multi-action applications. https://langchain.com\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44ca2770-c029-4c9d-8c4e-39bcdab57ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert in AI tweet fixer.\n",
    "Fix user's original tweet and based it on the mood.\n",
    "\n",
    "Original Tweet: {tweet}\n",
    "Mood: {mood}\n",
    "Fixed Tweet:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8034622-1d4a-4cfb-bcd1-334076b2f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_fixer = second_prompt | model | StrOutputParser()\n",
    "fixed_tweet = tweet_fixer.invoke({\"tweet\" : tweet, \"mood\" : \"funny\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32bccf70-8aa8-4e3f-a04d-41675603219e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"🚀 Hold onto your keyboards, developers! 🌟 Langchain just unleashed LangGraph, your new coding sidekick for building stateful, multi-action apps! It's like a Swiss Army knife, but for code! 🛠️💻 Time to level up and impress your cat with your skills! 😸✨ Check it out 👉 https://langchain.com #LangGraph #DevLife #CodingMagic\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5320f78-01ab-4bc4-bc66-0de1142ed7fe",
   "metadata": {},
   "source": [
    "### RunnableParallel\n",
    "Execute more than one `Runnable` components concurrently. It is particularly useful for tasks that can be executed independently, as it improves efficiency by running these tasks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5d8e933-8e93-4763-be22-f23eb7165a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17b981a1-997d-44b1-8a7c-b3b54339ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\") | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1ff15d8-d664-4dac-b99d-09febf864860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel(joke=chain1, short_poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e1aa187-4c18-44a1-b14e-cb317647ecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "response = parallel_chain.invoke({\"topic\" : \"bears\"})\n",
    "\n",
    "# the time measured is too tiny to tell the difference between parallel or batch chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2366dd62-446f-481d-91ab-125d142b27ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content='Why do bears have sticky paws?\\n\\nBecause they always paws for a moment before they swipe!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 13, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-a79454db-e0d9-4bf8-bfe7-4cbef0c86cc7-0', usage_metadata={'input_tokens': 13, 'output_tokens': 18, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'short_poem': AIMessage(content=\"In forest shadows, mighty bears roam free,  \\nNature's gentle giants, wild as they can be.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 17, 'total_tokens': 38, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-23382629-b605-4ff3-b09a-a68e50998dfb-0', usage_metadata={'input_tokens': 17, 'output_tokens': 21, 'total_tokens': 38, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64271ca4-838f-4d60-8516-4ed01a36349c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do bears have sticky paws?\\n\\nBecause they always paws for a moment before they swipe!'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['joke'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43f35d25-7e72-4f72-a099-96a4776297a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In forest shadows, mighty bears roam free,  \\nNature's gentle giants, wild as they can be.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['short_poem'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "279ac731-09db-4b89-8a23-53fde60c92ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            +--------------------------------+             \n",
      "            | Parallel<joke,short_poem>Input |             \n",
      "            +--------------------------------+             \n",
      "                   ***               ***                   \n",
      "                ***                     ***                \n",
      "              **                           **              \n",
      "+--------------------+              +--------------------+ \n",
      "| ChatPromptTemplate |              | ChatPromptTemplate | \n",
      "+--------------------+              +--------------------+ \n",
      "           *                                   *           \n",
      "           *                                   *           \n",
      "           *                                   *           \n",
      "    +------------+                      +------------+     \n",
      "    | ChatOpenAI |                      | ChatOpenAI |     \n",
      "    +------------+*                     +------------+     \n",
      "                   ***               ***                   \n",
      "                      ***         ***                      \n",
      "                         **     **                         \n",
      "            +---------------------------------+            \n",
      "            | Parallel<joke,short_poem>Output |            \n",
      "            +---------------------------------+            \n"
     ]
    }
   ],
   "source": [
    "parallel_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052ea36-eb3e-4231-9951-9a6d7be1541e",
   "metadata": {},
   "source": [
    "### RunnablePassthrough\n",
    "Serves as a simple utility for passing data through without modification. It is useful in a pipeline or chain of operations when you want to retain certain intermediate results or bypass specific steps without applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "282f81ea-ba22-418a-9148-ab954332d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34c2237a-bb74-42ce-9d31-9299dead67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"mood\" and \"prompt\" are passed as input to \"tweet_chain\"\n",
    "# RunnablePassthrough() makes both of these passes through unchanged\n",
    "# itemgetter extracts the value of mood parameter and assigned it to \"mood\"\n",
    "\n",
    "tweet_chain = RunnableParallel(\n",
    "    {\n",
    "        \"mood\": RunnablePassthrough() | itemgetter(\"mood\"),\n",
    "        \"tweet\": tweet_generator\n",
    "    }\n",
    ") | tweet_fixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a09bb890-07f0-4176-9ae1-807167537117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, just what we needed—another tool! 🙄 Langchain launched LangGraph, your “best friend” for building stateful, multi-action apps. Because clearly, our coding game wasn’t already complicated enough! 💻✨ Check it out if you dare: https://langchain.com #Langchain #LangGraph #CodingOverload #AppDevelopment'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_chain.invoke(\n",
    "    {\n",
    "        \"prompt\": \"Langchain releases LangGraph for building stateful, multi-action applications. https://langchain.com\",\n",
    "        \"mood\": \"sarcastic\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2adeb26-f41a-4927-b3f3-b329b82b29eb",
   "metadata": {},
   "source": [
    "### Batch\n",
    "Using this method can significantly improve performance when needing to process multiple independent inputs as the processing can be done in parallel instead of sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "393e5349-7532-4eba-af68-d1005104f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "response = parallel_chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n",
    "\n",
    "# the time measured is too tiny to tell the difference between parallel or batch chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07290588-e28d-4753-a5a2-db93e78c3d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do bears have hairy coats?\\n\\nBecause they look silly in sweaters!'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]['joke'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ddf7790-0d95-4b03-bc08-f5c7325a67fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In forests deep where shadows play,  \\nMighty bears roam wild, both fierce and gray.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]['short_poem'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02d08d48-10a0-41cf-9053-bc63ee166d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[1]['joke'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aa9905e0-d0b8-4ada-a36e-4149487dd216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Silent paws on moonlit floors,  \\nWhiskered dreams behind closed doors.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[1]['short_poem'].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b73f2b-7177-4405-b4b4-07ae081643f1",
   "metadata": {},
   "source": [
    "### Stream\n",
    "Streaming enhances the responsiveness of application by displaying the output progressively, even before a complete response is ready. Due the latency of LLMs response generation, streaming improves user experience (UX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4153b37-06f7-49da-8b89-ed392049558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Why |  did |  the |  dog |  sit |  in |  the |  shade | ?\n",
      "\n",
      " | Because |  he |  didn | ’t |  want |  to |  become |  a |  hot |  dog | ! |  | \n",
      "----------\n",
      " | In |  fields |  of |  green | , |  a |  joyful |  chase | , |   \n",
      " | A |  wag | ging |  tail | , |  love | 's |  pure |  embrace | . |  | \n"
     ]
    }
   ],
   "source": [
    "joke_str = \"\"\n",
    "short_poem_str = \"\"\n",
    "\n",
    "for s in parallel_chain.stream({\"topic\" : \"dog\"}):\n",
    "    if 'joke' in s:\n",
    "        joke_str = joke_str + s['joke'].content + \" | \"\n",
    "    elif 'short_poem' in s:\n",
    "        short_poem_str = short_poem_str + s['short_poem'].content + \" | \"\n",
    "\n",
    "print(joke_str)\n",
    "print('-'*10)\n",
    "print(short_poem_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9353a-a869-4d85-8460-ae67010e8fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
