{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1dd8cc-9979-4757-b5f5-21e4d5605d41",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/Day_02/L13/L13_Challenge.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e66413-b20b-45dc-88a7-725a8005916d",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab (preferred).\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `SST_DP2025`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181688a-e654-416d-b660-d6e876f047e7",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at Python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e0e74-9750-484e-85a3-0e2df89ab3eb",
   "metadata": {},
   "source": [
    "# Lesson 13 Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a1281-5590-4f9d-88fc-76185733bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langgraph\n",
    "%pip install --quiet -U langchain-openai\n",
    "%pip install --quiet -U langchain-community\n",
    "%pip isntall --quiet -U tavily-python\n",
    "%pip install --quiet -U wikipedia\n",
    "%pip install --quiet -U \"langchain-chroma>=0.1.2\"\n",
    "%pip install --quiet -U pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed6b64-6afd-49b1-b20d-c0126966cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain              0.3.11\n",
    "# langgraph              0.2.59\n",
    "# langchain-core         0.3.24\n",
    "# langchain-openai       0.2.12\n",
    "# langchain-community    0.3.12\n",
    "# openai                 1.57.2\n",
    "# pydantic               2.10.3\n",
    "# tavily-python          0.5.0\n",
    "# wikipedia              1.4.0\n",
    "# chroma                 0.5.23\n",
    "# PyMuPDF                1.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84b6a8-289a-49d6-ad78-0e105a5ff503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b5efe-52a1-473b-baaf-49428f5c8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03db0f-e170-40f3-ad72-35f68d3b44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goto https://tavily.com/ and sign up to get an API key\n",
    "\n",
    "# get Tavily Search API key ready and enter it when ask\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2c924-3ffd-495a-95f6-5b64468cede9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82d173-7455-4a74-a443-bae0f5e9826e",
   "metadata": {},
   "source": [
    "## Activity 1\n",
    "\n",
    "<img src=\"harrypotter.png\" width=\"70%\" height=\"auto\">\n",
    "\n",
    "We are building a Retrieval-Augmented Generation (RAG) application using an LLM to answer questions about Harry Potter the book \"The Sorcerer’s Stone\". Complete the code, construct the graph, and run tests to ensure functionality. Feel free to enhance the implementation.\n",
    "\n",
    "## <font color=\"#FF0000\">IMPORTANT</font>\n",
    "If you are running this code in Google Colab, you need to run the below commands to download `Harry Potter_The Sorcerers Stone.pdf` to Google Colab.\n",
    "\n",
    "Comment out the below code with '#' if you are not running it in your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50983116-5582-4fe0-af1b-210ba617611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://raw.githubusercontent.com/koayst-rplesson/SDGAI_LLMforGenAIApp_Labs/refs/heads/main/L13/Harry Potter_The Sorcerers Stone.pdf\"\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e981a1-5e3a-487d-b6a7-d481e6f882cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea6bdd-30f1-4932-a8e1-9a94d3d3f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "\n",
    "llm_model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79f026-5d8d-4fde-8ea4-efe3ecd8d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the PDF loader\n",
    "\n",
    "# TODO:  file name of the Harry Potter PDF book\n",
    "loader = PyMuPDFLoader(\"_____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261055f4-3df7-412f-9399-ec9c8fec45d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the PDF using the PDF loader\n",
    "\n",
    "hp_pages = []\n",
    "for page in loader.load():\n",
    "    hp_pages.append(page)\n",
    "\n",
    "# ignore the user warnings since pages 0 and 1 are blank pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88683c7-a2d2-435a-858f-ba319f75a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some information about the PDF\n",
    "\n",
    "print(f'We have {len(hp_pages)} pages in total\\n')\n",
    "print('-'*13)\n",
    "\n",
    "# print page 101th\n",
    "# TODO: Enter the index in hp_pages\n",
    "print(f'Page Content:\\n{\"-\"*13}\\n{hp_pages[_____].page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf7687-8eb6-4d7f-b54f-02a7890506ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the text splitter (chunking)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "# split the text\n",
    "texts = text_splitter.split_documents(hp_pages)\n",
    "print(f'We have created {len(texts)} chunks from {len(hp_pages)} pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608955e7-bcf9-4509-bf12-0f1aecaf2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vector store as a retriever\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name = \"rag-chroma\",\n",
    "    documents = texts,\n",
    "\n",
    "    # Note: \n",
    "    # if you use Chroma.from_documents, you should use 'embedding'\n",
    "    # if you use Chroma, you should use \"embedding_function'    \n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    persist_directory = \"./chroma_langchain_db\",\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f8b4e-6d63-48de-994b-1e6ef0a34fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the state of the graph \n",
    "\n",
    "# TODO: Enter the types of variables: question, generation and documents\n",
    "class GraphState(BaseModel):\n",
    "    question: Optional[_____] = None     # Optional string, default is None\n",
    "    generation: Optional[_____] = None   # Optional string, default is None\n",
    "    documents: List[_____] = []          # Defaults to an empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c0329-bd99-4eaa-a48c-cd8402ecfb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the system and human prompts\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say you don't know.\n",
    "Use maximum of three sentences and keep the answer concise.\n",
    "Only provide the answer and nothing else!\n",
    "\"\"\"\n",
    "\n",
    "human_prompt=\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helful assistant tasked with performing arithmetic operations.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af29b92-9204-4d59-a1ed-e57a09c7eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialise the system and human prompts\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", _____),\n",
    "        (\"human\", _____)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3f115-c980-4113-8c8c-234b4e41c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the rag pipeline\n",
    "\n",
    "# TODO: initialise the rag_chain using LCEL\n",
    "rag_chain = _____ | _____ | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cf97a-7271-418d-90ff-67c58bb49a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the retrieval node for the graph\n",
    "\n",
    "def retrieval_node(state: GraphState):\n",
    "    new_documents = retriever.invoke(state.question)\n",
    "    new_documents = [d.page_content for d in new_documents]\n",
    "    state.documents.extend(new_documents)\n",
    "    return {\"documents\" : state.documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea8dde-a967-4405-834c-117dc431068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the generation node for the graph\n",
    "\n",
    "def generation_node(state: GraphState): \n",
    "    generation = rag_chain.invoke({\n",
    "        \"context\" : \"\\n\\n\".join(state.documents),\n",
    "        \"question\" : state.question,\n",
    "    })\n",
    "\n",
    "    return {\"generation\" : generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83624ac8-fd56-4564-9886-44f066bd4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the graph\n",
    "\n",
    "# TODO: build the graph\n",
    "graph_builder = StateGraph(GraphState)\n",
    "\n",
    "graph_builder.add_node('retrieval_node', _____)\n",
    "graph_builder.add_node('generation_node', _____)\n",
    "\n",
    "graph_builder.add_edge(_____, 'retrieval_node')\n",
    "graph_builder.add_edge('_____', '_____')\n",
    "graph_builder.add_edge('generation_node', _____)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a261461-af71-4d21-8437-dfa90c32155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c554f-1134-4b9f-8198-ea7182e075e3",
   "metadata": {},
   "source": [
    "### Q&A Samples for Your Observation Comparison\n",
    "They were generated using online ChatGPT.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: How does Harry discover he is a wizard?**\n",
    "A: Harry discovers he is a wizard on his eleventh birthday when Hagrid visits him at the Dursleys' home. Hagrid delivers a letter from Hogwarts School of Witchcraft and Wizardry, explains Harry’s true heritage, and reveals that he is famous in the wizarding world for surviving Voldemort’s attack as a baby.\n",
    "\n",
    "**Q: What is the significance of the Sorcerer’s Stone, and why is it hidden at Hogwarts?**\n",
    "A: The Sorcerer’s Stone, created by Nicolas Flamel, has the power to grant immortality and produce unlimited gold. It is hidden at Hogwarts to keep it safe from Voldemort, who seeks it to regain his physical form and power. The stone is guarded by a series of magical challenges created by Hogwarts professors, including a giant three-headed dog named Fluffy and various enchanted obstacles.\n",
    "\n",
    "**Q: How does Harry ultimately defeat Professor Quirrell and prevent Voldemort from getting the Stone?**\n",
    "A: Harry confronts Professor Quirrell, who is possessed by Voldemort, in the final chamber. When Quirrell tries to seize the Sorcerer’s Stone, he cannot touch Harry due to the protective magic left by Harry’s mother’s sacrifice. This love-based protection causes Quirrell immense pain and ultimately leads to Voldemort fleeing, leaving Harry victorious.\n",
    "\n",
    "**Q: What role does the Mirror of Erised play in the story?**\n",
    "A: The Mirror of Erised shows the viewer their deepest desires. For Harry, it displays his parents, highlighting his longing for family. The mirror serves as a tool for self-reflection and an important plot device when Dumbledore uses it to hide the Sorcerer’s Stone. Only someone who seeks the Stone without selfish intent can retrieve it, a test Voldemort fails.\n",
    "\n",
    "**Q: How does Harry ultimately defeat Professor Quirrell and prevent Voldemort from getting the Stone?**\n",
    "A: Harry confronts Professor Quirrell, who is possessed by Voldemort, in the final chamber. When Quirrell tries to seize the Sorcerer’s Stone, he cannot touch Harry due to the protective magic left by Harry’s mother’s sacrifice. This love-based protection causes Quirrell immense pain and ultimately leads to Voldemort fleeing, leaving Harry victorious.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45917890-e597-4901-8739-5406d6acf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question to be asked by user\n",
    "\n",
    "user_input = {'question' : 'How does Harry ultimately defeat Professor Quirrell and prevent Voldemort from getting the Stone?'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e836ef6-61f0-4b58-8372-3c2ec9f2fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the graph to generate the output\n",
    "\n",
    "for chunk in graph.stream(user_input, stream_mode='updates'):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values)\n",
    "        print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a28b7-8e55-4df9-8897-f1f3015fe42a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc6f41-5fd5-4ab8-a238-1c10205fc72f",
   "metadata": {},
   "source": [
    "## Activity 2\n",
    "\n",
    "<img src=\"Wikipedia_Logo_1.0.png\" width=\"20%\" height=\"auto\">\n",
    "\n",
    "We will be using Wikipedia and a Google search services to gather contexts to answer a question directed to the LLM. You are to complete the code and build the graph. Run test on it to ensure that it is working. Feel free to improve it.\n",
    "\n",
    "**NOTE**</br>\n",
    "Make sure you have signed up at [Tavily](https://tavily.com/) and get an API key before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be6546-ab80-4fe5-b495-a21b3219f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from typing import Annotated\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122513e-b407-44cf-877f-2b9fcb4e0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 'gpt-4o-mini`\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a863e2c-750c-4aff-9330-bfd37830905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the code\n",
    "\n",
    "# TODO: Enter the types of question and answe\n",
    "class State(TypedDict):\n",
    "    # the question to be send to the LLM\n",
    "    question: _____\n",
    "\n",
    "    # the answer returned by the LLM\n",
    "    answer: _____\n",
    "\n",
    "    # a list of context is created\n",
    "    # from the searches \n",
    "    context: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d9eb0-5044-43ef-8286-b7cfe2274b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_web node is defined as below\n",
    "\n",
    "def search_web(state):\n",
    "    \"\"\" Retrieve docs from web search \"\"\"\n",
    "    tavily_search = TavilySearchResults(max_results=3)    # return three search result\n",
    "\n",
    "    # TODO: what needs to pass in to invoke the tavily search\n",
    "    search_docs = tavily_search.invoke(state['_____'])\n",
    "\n",
    "    # format the search result\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>' for doc in search_docs            \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\" : [formatted_search_docs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf982b7-1fc8-4c8c-b040-bbaa264e0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_wikipedia node is defined as below\n",
    "\n",
    "def search_wikipedia(state):\n",
    "    \n",
    "    \"\"\" Retrieve docs from wikipedia \"\"\"\n",
    "    # Search\n",
    "    search_docs = WikipediaLoader(\n",
    "\n",
    "        #TODO: what question to ask?\n",
    "        query=state['_____'], \n",
    "        load_max_docs=2).load()\n",
    "\n",
    "    # format the search result\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd17c4-f79e-41d9-be70-2f2dce85fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_answer node is defined as below\n",
    "\n",
    "def generate_answer(state):\n",
    "    \"\"\" Node to generate answer \"\"\"\n",
    "\n",
    "    # get context and question from state\n",
    "    # TODO: initialise both context and question\n",
    "    context = state['_____']\n",
    "    question = state['_____']\n",
    "\n",
    "    answer_template = \"\"\"Answer the question: {question} using this context: {context}\"\"\"\n",
    "    answer_instructions = answer_template.format(question = question, context = context)\n",
    "\n",
    "    # Now get the model (LLM) to answer\n",
    "    answer = model.invoke(\n",
    "        [SystemMessage(content = answer_instructions)] +\n",
    "        [HumanMessage(content = \"Please answer the question and indicate the source of truth.\")]\n",
    "    )\n",
    "\n",
    "    return {\"answer\" : answer}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1219420-b1db-4c46-b3df-6eff7c4cb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "\n",
    "# TODO: build the graph\n",
    "# initialise the state graph\n",
    "builder = StateGraph(_____)\n",
    "\n",
    "# setup the nodes\n",
    "builder.add_node(\"_____\", search_web)\n",
    "builder.add_node(\"_____\", search_wikipedia)\n",
    "builder.add_node(\"_____\", generate_answer)\n",
    "\n",
    "# connect the nodes\n",
    "builder.add_edge(_____, \"_____\")\n",
    "builder.add_edge(_____, \"_____\")\n",
    "builder.add_edge(\"_____\", \"_____\")\n",
    "builder.add_edge(\"_____\", \"_____\")\n",
    "builder.add_edge(\"_____\", _____)\n",
    "\n",
    "#compile the graph as assistant\n",
    "assistant = builder.compile()\n",
    "\n",
    "# display the graph\n",
    "display(Image(assistant.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c65d33-b899-4296-8dc6-ebbc52d09076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the graph to send the question to the LLM\n",
    "# question is \"How were Nvidia's Q2 2024 earnings?\"\n",
    "# once you got it running feel free to invoke the \n",
    "# graph with your own question\n",
    "\n",
    "# TODO: fill in the blanks\n",
    "result = assistant.invoke({\"_____\": \"_____?\"})\n",
    "\n",
    "# print the answer to the question\n",
    "print(result['answer'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f0a21-6fb2-483d-8ce9-f8e44faa09dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0369c4-4269-4744-8940-4ebcd96603ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
